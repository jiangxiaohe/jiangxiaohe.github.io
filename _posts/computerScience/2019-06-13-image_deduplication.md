---
layout: post
title: 图片去重
tags:
categories: computerScience
description: 图片如何去重？
---

[TOC]

应用于图片网站的版权保护，在用户上传图片时与媒体库中的图片进行比较，如果检测出该图片已存在，就可以报错，并标记该用户欺诈。

在大量文件中检测某文件是否已经存在的一个常用方法是，通过计算数据集中每一个文件的哈希值，并将该哈希值存储在数组库中。当想要查找某特定文件时，首先计算该文件哈希值，然后在数据库中查找该哈希值。

md5方法对于文件的任何细微改变都会导致hash值的不同，不用于图片去重，常用的方法是dhash。

[Google相似图片搜索的原理概述:基于感知hash](http://www.woshipm.com/ucd/150245.html)
> Google “相似图片搜索” : 根据Neal Krawetz博士的解释，实现相似图片搜素的关键技术叫做”感知哈希算法”（Perceptualhash algorithm），它的作用是对每张图片生成一个”指纹”（fingerprint）字符串，然后比较不同图片的指纹。结果越接近，就说明图片越相似。


# 海量数据去重之SimHash算法

[CSDN/u010454030](https://blog.csdn.net/u010454030/article/details/49102565)

SimHash是Google在2007年发表的论文《Detecting Near-Duplicates for Web Crawling 》中提到的一种指纹生成算法或者叫指纹提取算法，被Google广泛应用在亿级的网页去重的Job中，作为locality sensitive hash（局部敏感哈希）的一种，其主要思想是降维。

一篇若干数量的文本内容，经过simhash降维后，可能仅仅得到一个长度为32或64位的二进制由01组成的字符串，对该文本的simhash码过程请查看原文，主要步骤如下：
* 清洗+hash、关键词hash、关键词向量加权建立、合并向量累加、降维得指纹
* 得到指纹后通过汉明距离比较文本相似度。在google的论文给出的数据中，64位的签名，在海明距离为3的情况下，可认为两篇文档是相似的或者是重复的，当然这个值只是参考值，针对自己的应用可能又不同的测试取值

到这里相似度问题基本解决，但是按这个思路，在海量数据几百亿的数量下，效率问题还是没有解决的，因为数据是不断添加进来的，不可能每来一条数据，都要和全库的数据做一次比较，按照这种思路，处理速度会越来越慢，线性增长。

针对海量数据的去重效率，我们可以将64位指纹，切分为4份16位的数据块，根据抽屉原理在海明距离为3的情况，如果两个文档相似，那么它必有一个块的数据是相等的。

然后将4份数据通过K-V数据库或倒排索引存储起来K为16位截断指纹，V为K相等时剩余的48位指纹集合，查询时候，精确匹配这个指纹的4个16位截断。

> 我们用dhash的64位hash串，汉明距离在5位及以下可认为重复，采用simhash算法，分成六块。

# 相似搜索
