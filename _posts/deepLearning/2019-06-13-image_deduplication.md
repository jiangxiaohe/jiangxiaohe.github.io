---
layout: post
title: 图片去重
tags:
categories: deepLearning
description: 图片如何去重？
---

应用于图片网站的版权保护，在用户上传图片时与媒体库中的图片进行比较，如果检测出该图片已存在，就可以报错，并标记该用户欺诈。

在大量文件中检测某文件是否已经存在的一个常用方法是，通过计算数据集中每一个文件的哈希值，并将该哈希值存储在数组库中。当想要查找某特定文件时，首先计算该文件哈希值，然后在数据库中查找该哈希值。

md5方法对于文件的任何细微改变都会导致hash值的不同，不用于图片去重，常用的方法是dhash。

[Google相似图片搜索的原理概述:基于感知hash](http://www.woshipm.com/ucd/150245.html)
> Google “相似图片搜索” : 根据Neal Krawetz博士的解释，实现相似图片搜素的关键技术叫做”感知哈希算法”（Perceptualhash algorithm），它的作用是对每张图片生成一个”指纹”（fingerprint）字符串，然后比较不同图片的指纹。结果越接近，就说明图片越相似。


# 海量数据去重之SimHash算法

[CSDN/u010454030](https://blog.csdn.net/u010454030/article/details/49102565)

SimHash是Google在2007年发表的论文《Detecting Near-Duplicates for Web Crawling 》中提到的一种指纹生成算法或者叫指纹提取算法，被Google广泛应用在亿级的网页去重的Job中，作为locality sensitive hash（局部敏感哈希）的一种，其主要思想是降维。

一篇若干数量的文本内容，经过simhash降维后，可能仅仅得到一个长度为32或64位的二进制由01组成的字符串，对该文本的simhash码过程请查看原文，主要步骤如下：
* 清洗+hash、关键词hash、关键词向量加权建立、合并向量累加、降维得指纹
* 得到指纹后通过汉明距离比较文本相似度。在google的论文给出的数据中，64位的签名，在海明距离为3的情况下，可认为两篇文档是相似的或者是重复的，当然这个值只是参考值，针对自己的应用可能又不同的测试取值

到这里相似度问题基本解决，但是按这个思路，在海量数据几百亿的数量下，效率问题还是没有解决的，因为数据是不断添加进来的，不可能每来一条数据，都要和全库的数据做一次比较，按照这种思路，处理速度会越来越慢，线性增长。

针对海量数据的去重效率，我们可以将64位指纹，切分为4份16位的数据块，根据抽屉原理在海明距离为3的情况，如果两个文档相似，那么它必有一个块的数据是相等的。

然后将4份数据通过K-V数据库或倒排索引存储起来K为16位截断指纹，V为K相等时剩余的48位指纹集合，查询时候，精确匹配这个指纹的4个16位截断。

> 我们用dhash的64位hash串，汉明距离在5位及以下可认为重复，采用simhash算法，分成六块。

# 相似搜索

相似搜索的共有三大类方法：
* 基于树的方法
	* KD树是其下的经典算法。一般而言，在空间维度比较低时，KD树的查找性能还是比较高效的；但当空间维度较高时，该方法会退化为暴力枚举，性能较差，这时一般会采用下面的哈希方法或者矢量量化方法。
* 哈希方法
	* LSH(Locality-Sensitive Hashing)是其下的代表算法。文献[7]是一篇非常好的LSH入门资料。
	* 对于小数据集和中规模的数据集(几个million-几十个million)，基于LSH的方法的效果和性能都还不错。这方面有2个开源工具FALCONN和NMSLIB。
* 矢量量化方法
	* 矢量量化方法，即vector quantization。在矢量量化编码中，关键是码本的建立和码字搜索算法。比如常见的聚类算法，就是一种矢量量化方法。而在相似搜索中，向量量化方法又以PQ方法最为典型。
	* 对于大规模数据集(几百个million以上)，基于矢量量化的方法是一个明智的选择，可以用用Faiss开源工具。
