---
layout: post
title: cuda配置、例程
tags:
categories: deepLearning
description: 实验笔记
---

# 安装cuda

在Ubuntu16.04上安装CUDA9.0

有很多坑，主要参考文章：

[简书deb](https://www.jianshu.com/p/71bc5f02ecd2)
[CSDN](https://blog.csdn.net/qlulibin/article/details/78714596)
[简书](https://www.jianshu.com/p/7acb5688f2dd)

[官方文档](https://developer.download.nvidia.cn/compute/cuda/9.0/Prod/docs/sidebar/CUDA_Installation_Guide_Linux.pdf)

# 在线deb包安装方式
**亲测没有问题**

1. 禁用nouveau驱动

```
1创建并修改文件
sudo vim /etc/modprobe.d/blacklist-nouveau.conf
2在文本中输入：
blacklist nouveau
options nouveau modeset=0
3执行生效
sudo update-initramfs -u
4查看是否关闭
lsmod | grep nouveau
若无内容输出，则成功关闭，往往要重启下才行
```

2. 验证系统是否安装了GCC
`gcc --version`

3. 验证系统是否安装了kernel header和 package development（若这里未安装安装后需要重启并ctrl+alt+F1进入命令行界面，不要登录用户界面
`uname -r`
`sudo apt-get install linux-headers-$(uname -r)`

4. 关闭图形界面
`sudo service lightdm stop`
`service --status-all | grep lightdm`

5. [在线deb方式安装cuda](https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&target_distro=Ubuntu&target_version=1604&target_type=debnetwork)
	* 上述官网下载在线安装脚本
	* sudo dpkg -i cuda-repo-ubuntu1604_10.1.168-1_amd64.deb
	* sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/
	compute/cuda/repos/<distro>/<architecture>/7fa2af80.pub
	* sudo apt-get update
	* sudo apt-get install cuda-9.0
	* **这里一定要加9.0，第一次没有加，直接安装了最新版10.1**
	* 卸载10.1命令：sudo apt-get remove cuda、sudo apt autoremove
6. 检查Device Node Verification：`ls /dev/nvidia*`正确显示结果是`/dev/nvidia0       /dev/nvidiactl       /dev/nvidia-uvm`
	* 如果出现结果`ls: cannot access/dev/nvidia*: No such file or directory`，则需要编辑文件`sudo vi /etc/rc.local`。初次打开是空的，加入以下内容：

```
#!/bin/bash
/sbin/modprobe nvidia
if [ "$?" -eq 0 ]; then
 # Count the number of NVIDIA controllers found.
 NVDEVS=`lspci | grep -i NVIDIA`
 N3D=`echo "$NVDEVS" | grep "3D controller" | wc -l`
 NVGA=`echo "$NVDEVS" | grep "VGA compatible controller" | wc -l`
 N=`expr $N3D + $NVGA - 1`
 for i in `seq 0 $N`; do
 mknod -m 666 /dev/nvidia$i c 195 $i
 done
 mknod -m 666 /dev/nvidiactl c 195 255
else
 exit 1
fi
/sbin/modprobe nvidia-uvm
if [ "$?" -eq 0 ]; then
 # Find out the major device number used by the nvidia-uvm driver
 D=`grep nvidia-uvm /proc/devices | awk '{print $1}'`
 mknod -m 666 /dev/nvidia-uvm c $D 0
else
 exit 1
fi
```

7. 设置环境变量`sudo vim /etc/profile`.在结尾加入两行：

```
export PATH=/usr/local/cuda-9.0/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64\
${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}
```
重启电脑，检查上述的环境变量是否设置成功

8. 验证驱动版本：`cat /proc/driver/nvidia/version`
9. 验证CUDA Toolkit：`nvcc -V`
10. 尝试编译cuda提供的例子。
	* 先下载例子cuda-install-samples-9.0.sh <dir>
	* 在NVIDIA_CUDA-9.0_Samples下make。系统就会自动进入到编译过程，整个过程大概需要十几到二十分钟，请耐心等待。如果出现错误的话，系统会立即报错停止。如果编译成功，最后会显示Finished building CUDA samples
	* cd /home/xxx/cuda_samples/NVIDIA_CUDA-9.0_Samples/bin/x86_64/linux/release
	* ./deviceQuery和./bandwidthTest命令均显示PASS则表示成功
11. 查看cuda版本cat /usr/local/cuda/version.txt

# run文件方法
**该方法试了不行，不知道哪里出问题，nvidia-smi找不到驱动**
走了一些弯路，记录如下

1. 安装cuda并不需要更换linux内核，更换的步骤如下：
需要更换Linux内核，我的当前内核版本为4.15，需要换到4.4
* `uname -a`查看当前使用的内核版本
* 查看当前内核版本`dpkg -l|grep linux-image`
* 查看可以更新的内核版本`sudo apt-cache search linux-image`
* 安装新内核
* `sudo apt-get install linux-image-4.4.0-77-generic linux-image-extra-4.4.0-77-generic`
* 更新grub引导`sudo update-grub`
* 重启进入ubuntu高级选项
* 若开机没有进入grub引导界面，需要设置配置文件
	* `sudo vim /etc/default/grub`
	* 把带有hidden项都给注释掉
	* 把`grub_cmdline_linux_default`修改为`"text"`
	* `sudo update-grub`
* 重启后发现grub已经更换完毕，将grub设置复原

2. 禁用nouveau驱动
3. 验证系统是否安装了GCC
4. 验证系统是否安装了kernel header和 package development
5. 关闭图形化界面
6. 安装cuda

* `sudo sh cuda.run`

不安装OpenGL，顺便安装NVIDIA显卡驱动。即accept、n、y、y、y

最后显示installed，表示成功了

输入 $ sudo service lightdm start 重新启动图形化界面，没有循环登录则安装成功

* 如果之前安装驱动失败，先卸载

`sudo apt remove --purge nvidia*`

7. 重启电脑，检查Device Node Verification` ls /dev/nvidia*`
8. 设置环境变量
9. 验证驱动版本`cat /proc/driver/nvidia/version`
10. 验证CUDA Toolkit`nvcc -V`
11. 尝试编译cuda提供的例子
如果显示Result = PASS代表成功,若失败 Result = FAIL。这里的提示信息是：`no CUDA-capable device is detected`

**事实是，用上面的方法装了两次都不行，心灰意冷，听从师兄建议，改用在线deb包方式安装，成功**

# 其他问题

* 屏幕分辨率过低且不可调节

```
sudo mv /etc/X11/xorg.conf /etc/X11/xorg.conf.backup
sudo touch /etc/X11/xorg.conf
sudo reboot
```

# 安装CUDNN
* 主要看[官方文档](https://docs.nvidia.com/deeplearning/sdk/cudnn-install/#installlinux-tar)
* 官网下载对应版本cuDNN Library for Linux(cudnn-9.0-linux-x64-v7.tgz)需[登录下载](https://developer.nvidia.com/rdp/cudnn-archive)
* tar -xzvf cudnn-9.0-linux-x64-v7.tgz
* sudo cp cuda/include/cudnn.h /usr/local/cuda/include
* sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64
* sudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn*
* 查看cudnn版本号cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2
* 安装成功


# `nvidia-smi`命令
提供监控GPU使用情况和更改GPU状态的功能，是一个跨平台工具，它支持所有标准的NVIDIA驱动程序支持的Linux发行版以及从WindowsServer 2008 R2开始的64位的系统。

`nvidia-smi`命令显示所有GPU的当前状态信息：
* fan：风扇转速
* temp：GPU温度，GPU温度过高会导致GPU频率下降
* perf：性能状态，从P0(最大性能)到P12(最小性能)
* Pwr：GPU功耗
* Persistence-M：持续模式的状态（持续模式耗能大，但在新的GPU应用启动时花费时间更少）
* Bus-Id：GPU总线，domain:bus:device.function
* Disp.A：Display Active，表示GPU的显示是否初始化
* Memory-Usage：显存使用率
* Volatile GPU-Util：GPU使用率
* ECC：是否开启错误检查和纠正技术，0/DISABLED, 1/ENABLED
* Compute M.：计算模式，0/DEFAULT,1/EXCLUSIVE_PROCESS,2/PROHIBITED

* `nvidia-smi -l xxx`动态刷新，不写xxx时默认5s刷新一次
* `nvidia-smi -f xxx`将查询的信息输出到具体的文本中，不在终端显示
* `nvidia-smi -q`查询所有GPU的当前详细信息

手动修改GPU设备选项：
* `nvidia-smi –pm 0/1`,设置持久模式：0/DISABLED,1/ENABLED
* 待补充

[参考](https://blog.csdn.net/handsome_bear/article/details/80903477)

# [vscode中添加cu文件高亮](http://www.makaidong.com/combfish/3694_23320004.html)

* 问题：`*.cu`在VScode不能像`*.cc`或`*.cpp`一样在c++及c++ intelligence插件有关键字的高亮以及go to definition等的操作
* 解决方案：添加*.cu与*.cpp文件的关联

1. VScode： File→Preferences→Setting
2. 在搜索框中输入 “ files.associations”，得到查找结果
3. 在右边的窗口添加  `"files.associations": {"*.cu": "cpp"}`, 保存后即可看到*.cu文件可以像一般的cpp文件操作了



# cuda入门教程

运行程序前添加命令 `CUDA_VISIBLE_DEVICES=0` ，可以使得只有0号设备可用。也可以用命令 `CUDA_VISIBLE_DEVICES=0,2,3`

[CUDA官方入门教程](https://devblogs.nvidia.com/even-easier-introduction-cuda/)

[如何学习cuda/知乎小小将](https://zhuanlan.zhihu.com/p/34587739)

GPU并不是一个独立运行的计算平台，而需要与CPU协同工作，可以看成是CPU的协处理器，因此当我们在说GPU并行计算时，其实是指的基于CPU+GPU的异构计算架构。

PU包括更多的运算核心，其特别适合数据并行的计算密集型任务，如大型矩阵运算，而CPU的运算核心较少，但是其可以实现复杂的逻辑运算，因此其适合控制密集型任务。另外，CPU上的线程是重量级的，上下文切换开销大，但是GPU由于存在很多核心，其线程是轻量级的。因此，基于CPU+GPU的异构计算平台可以优势互补，CPU负责处理逻辑复杂的串行程序，而GPU重点处理数据密集型的并行计算程序，从而发挥最大功效。

CUDA是NVIDIA公司所开发的GPU编程模型，它提供了GPU编程的简易接口，基于CUDA编程可以构建基于GPU计算的应用程序。CUDA提供了对其它编程语言的支持，如C/C++，Python，Fortran等语言.

CUDA编程模型是一个异构模型，需要CPU和GPU协同工作。在CUDA中，host和device是两个重要的概念，我们用host指代CPU及其内存，而用device指代GPU及其内存。CUDA程序中既包含host程序，又包含device程序，它们分别在CPU和GPU上运行。同时，host与device之间可以进行通信，这样它们之间可以进行数据拷贝。典型的CUDA程序的执行流程如下：

1. 分配host内存，并进行数据初始化；
2. 分配device内存，并从host将数据拷贝到device上；
3. 调用CUDA的核函数在device上完成指定的运算；
4. 将device上的运算结果拷贝到host上；
5. 释放device和host上分配的内存。

上面流程中最重要的一个过程是调用CUDA的核函数来执行并行计算，kernel是CUDA中一个重要的概念，kernel是在device上线程中并行执行的函数，核函数用`__global__`符号声明，在调用时需要用`<<<grid, block>>>`来指定kernel要执行的线程数量，在CUDA中，每一个线程都要执行核函数，并且每个线程会分配一个唯一的线程号thread ID，这个ID值可以通过核函数的内置变量threadIdx来获得。

由于GPU实际上是异构模型，所以需要区分host和device上的代码，在CUDA中是通过函数类型限定词开区别host和device上的函数，主要的三个函数类型限定词如下：

* __global__：在device上执行，从host中调用（一些特定的GPU也可以从device上调用），返回类型必须是void，不支持可变参数参数，不能成为类成员函数。注意用__global__定义的kernel是异步的，这意味着host不会等待kernel执行完就执行下一步。
* __device__：在device上执行，单仅可以从device中调用，不可以和__global__同时用。
* __host__：在host上执行，仅可以从host上调用，一般省略不写，不可以和__global__同时用，但可和__device__，此时函数会在device和host都编译。

要深刻理解kernel，必须要对kernel的线程层次结构有一个清晰的认识。首先GPU上很多并行化的轻量级线程。kernel在device上执行时实际上是启动很多线程，一个kernel所启动的所有线程称为一个网格（grid），同一个网格上的线程共享相同的全局内存空间，grid是线程结构的第一层次，而网格又可以分为很多线程块（block），一个线程块里面包含很多线程，这是第二个层次。线程两层组织结构如下图所示，这是一个gird和block均为2-dim的线程组织。grid和block都是定义为dim3类型的变量，dim3可以看成是包含三个无符号整数（x，y，z）成员的结构体变量，在定义时，缺省值初始化为1。因此grid和block可以灵活地定义为1-dim，2-dim以及3-dim结构，对于图中结构（主要水平方向为x轴），定义的grid和block如下所示，kernel在调用时也必须通过执行配置<<<grid, block>>>来指定kernel所使用的线程数及结构。

```
dim3 grid(3, 2);
dim3 block(5, 3);
kernel_fun<<< grid, block >>>(prams...);
```

![kernel](https://pic1.zhimg.com/80/v2-aa6aa453ff39aa7078dde59b59b512d8_hd.jpg)

所以，一个线程需要两个内置的坐标变量（blockIdx，threadIdx）来唯一标识，它们都是dim3类型变量，其中blockIdx指明线程所在grid中的位置，而threaIdx指明线程所在block中的位置，如图中的Thread (1,1)满足：

```
threadIdx.x = 1
threadIdx.y = 1
blockIdx.x = 1
blockIdx.y = 1
```

一个线程块上的线程是放在同一个流式多处理器（SM)上的，但是单个SM的资源有限，这导致线程块中的线程数是有限制的，现代GPUs的线程块可支持的线程数可达1024个。有时候，我们要知道一个线程在blcok中的全局ID，此时就必须还要知道block的组织结构，这是通过线程的内置变量blockDim来获得。它获取线程块各个维度的大小。**对于一个2-dim的block $(D_x,D_y)$，线程 $(x,y)$ 的ID值为 $(x+y*D_x)$ ，如果是3-dim的block  $(D_x,D_y,D_z)$，线程 $(x,y,z)$ 的ID值为 $(x+y*D_x+z*D_x*D_y)$。另外线程还有内置变量gridDim，用于获得网格块各个维度的大小。**

kernel的这种线程组织结构天然适合vector,matrix等运算，如我们将利用上图2-dim结构实现两个矩阵的加法，每个线程负责处理每个位置的两个元素相加，代码如下所示。线程块大小为(16, 16)，然后将N*N大小的矩阵均分为不同的线程块来执行加法运算。

```Python
// Kernel定义
__global__ void MatAdd(float A[N][N], float B[N][N], float C[N][N])
{
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    int j = blockIdx.y * blockDim.y + threadIdx.y;
    if (i < N && j < N)
        C[i][j] = A[i][j] + B[i][j];
}
int main()
{
    ...
    // Kernel 线程配置
    dim3 threadsPerBlock(16, 16);
    dim3 numBlocks(N / threadsPerBlock.x, N / threadsPerBlock.y);
    // kernel调用
    MatAdd<<<numBlocks, threadsPerBlock>>>(A, B, C);
    ...
}
```

此外这里简单介绍一下CUDA的内存模型，如下图所示。可以看到，每个线程有自己的私有本地内存（Local Memory），而每个线程块有包含共享内存（Shared Memory）,可以被线程块中所有线程共享，其生命周期与线程块一致。此外，所有的线程都可以访问全局内存（Global Memory）。还可以访问一些只读内存块：常量内存（Constant Memory）和纹理内存（Texture Memory）。内存结构涉及到程序优化，这里不深入探讨它们。

![CUDA内存模型](https://pic2.zhimg.com/80/v2-6456af75530956da6bc5bab7418ff9e5_hd.jpg)

GPU硬件的一个核心组件是SM，前面已经说过，SM是英文名是 Streaming Multiprocessor，翻译过来就是流式多处理器。SM的核心组件包括CUDA核心，共享内存，寄存器等，SM可以并发地执行数百个线程，并发能力就取决于SM所拥有的资源数。当一个kernel被执行时，它的gird中的线程块被分配到SM上，一个线程块只能在一个SM上被调度。SM一般可以调度多个线程块，这要看SM本身的能力。那么有可能一个kernel的各个线程块被分配多个SM，所以grid只是逻辑层，而SM才是执行的物理层。SM采用的是SIMT (Single-Instruction, Multiple-Thread，单指令多线程)架构，基本的执行单元是线程束（wraps)，线程束包含32个线程，这些线程同时执行相同的指令，但是每个线程都包含自己的指令地址计数器和寄存器状态，也有自己独立的执行路径。所以尽管线程束中的线程同时从同一程序地址执行，但是可能具有不同的行为，比如遇到了分支结构，一些线程可能进入这个分支，但是另外一些有可能不执行，它们只能死等，因为GPU规定线程束中所有线程在同一周期执行相同的指令，线程束分化会导致性能下降。当线程块被划分到某个SM上时，它将进一步划分为多个线程束，因为这才是SM的基本执行单元，但是一个SM同时并发的线程束数是有限的。这是因为资源限制，SM要为每个线程块分配共享内存，而也要为每个线程束中的线程分配独立的寄存器。所以SM的配置会影响其所支持的线程块和线程束并发数量。总之，就是网格和线程块只是逻辑划分，一个kernel的所有线程其实在物理层是不一定同时并发的。所以kernel的grid和block的配置不同，性能会出现差异，这点是要特别注意的。还有，由于SM的基本执行单元是包含32个线程的线程束，所以block大小一般要设置为32的倍数。

编译并运行C++程序add.`g++ add.cpp -o add.out` `./add.out`

在进行CUDA编程前，可以先检查一下自己的GPU的硬件配置，这样才可以有的放矢，可以通过下面的程序获得GPU的配置属性：

```cpp
		int dev = 0;
		cudaDeviceProp devProp;
		CHECK(cudaGetDeviceProperties(&devProp, dev));
		std::cout << "使用GPU device " << dev << ": " << devProp.name << std::endl;
		std::cout << "SM的数量：" << devProp.multiProcessorCount << std::endl;
		std::cout << "每个线程块的共享内存大小：" << devProp.sharedMemPerBlock / 1024.0 << " KB" << std::endl;
		std::cout << "每个线程块的最大线程数：" << devProp.maxThreadsPerBlock << std::endl;
		std::cout << "每个EM的最大线程数：" << devProp.maxThreadsPerMultiProcessor << std::endl;
		std::cout << "每个EM的最大线程束数：" << devProp.maxThreadsPerMultiProcessor / 32 << std::endl;
```

> 还不清楚该程序怎么跑起来？

内存管理函数包括：
* cudaError_t cudaMalloc(void** devPtr, size_t size);
* cudaError_t cudaMemcpy(void* dst, const void* src, size_t count, cudaMemcpyKind kind)


# 向量加法实例

```cpp
#include <iostream>
#include <math.h>
#include <ctime>
#include <cstdio>

// function to add the elements of two arrays
void add(int n, float *x, float *y)
{
  for (int i = 0; i < n; i++)
      y[i] = x[i] + y[i];
}

int main(void)
{
  int N = 1<<20; // 1M elements

  float * x = new float[N];
  float * y = new float[N];

  // initialize x and y arrays on the host
  for (int i = 0; i < N; i++) {
    x[i] = 1.0f;
    y[i] = 2.0f;
  }

  // Run kernel on 1M elements on the CPU
  clock_t start = clock();
  add(N, x, y);
  clock_t finish = clock();
  double duration = (double)(finish - start) / CLOCKS_PER_SEC;
  printf("%f seconds\n",duration);

  // Check for errors (all values should be 3.0f)
  float maxError = 0.0f;
  for (int i = 0; i < N; i++)
    maxError = fmax(maxError, fabs(y[i]-3.0f));
  std::cout << "Max error: " << maxError << std::endl;

  // Free memory
  delete [] x;
  delete [] y;

  return 0;
}
```

C++ 测试时间：0.004925 seconds

修改为CUDA：
1. 将说明符`__global__`添加到函数中，该函数告诉cuda编译器这是一个在GPU上运行并可以在CPU调用的函数。这些  `__global__` 函数称为内核。
2. cuda中的内存分配。在GPU中进行计算需要分配GPU可访问的内存。通过调用`cudaMallocManaged()`获得一个可以供CPU或GPU调用的指针，要释放该指针，只需要将指针传递给`cudaFree()`.因此需要将上面代码中的new和delete分别更换如下：

```cpp
// Allocate Unified Memory -- accessible from CPU or GPU
float *x, *y;
cudaMallocManaged(&x, N*sizeof(float));
cudaMallocManaged(&y, N*sizeof(float));
// Free memory
cudaFree(x);
cudaFree(y);
```

3. 在GPU启动add()，用三角括号语法`<<<>>>`指定CUDA内核启动。`add<<<1, 1>>>(N, x, y);`。需要注意的是，cuda内核并不会阻塞CPU线程，因此CPU要等GPU完成后才能访问结果，需在CPU进行错误检查之前调用`cudaDeviceSynchronize()`

4. 完整代码如下，注意此代码保存为`.cu`格式的cuda文件。

```cpp
#include <iostream>
#include <math.h>
#include <ctime>
#include <cstdio>

// Kernel function to add the elements of two arrays
__global__
void add(int n, float *x, float *y)
{
  for (int i = 0; i < n; i++)
    y[i] = x[i] + y[i];
}

int main(void)
{
  int N = 1<<20;
  float * x, * y;

  // Allocate Unified Memory – accessible from CPU or GPU
  cudaMallocManaged(&x, N*sizeof(float));
  cudaMallocManaged(&y, N*sizeof(float));

  // initialize x and y arrays on the host
  for (int i = 0; i < N; i++) {
    x[i] = 1.0f;
    y[i] = 2.0f;
  }

  // Run kernel on 1M elements on the GPU
  clock_t start = clock();
  add<<<1, 1>>>(N, x, y);
  clock_t finish = clock();
  double duration = (double)(finish - start) / CLOCKS_PER_SEC;
  printf("%f seconds\n",duration);

  // Wait for GPU to finish before accessing on host
  cudaDeviceSynchronize();

  // Check for errors (all values should be 3.0f)
  float maxError = 0.0f;
  for (int i = 0; i < N; i++)
    maxError = fmax(maxError, fabs(y[i]-3.0f));
  std::cout << "Max error: " << maxError << std::endl;

  // Free memory
  cudaFree(x);
  cudaFree(y);

  return 0;
}
```

5. 编译运行`nvcc add.cu -o add_cuda` `./add_cuda`

运行时间：0.000024 seconds

6. 其实，并不需要调用ctime库来查看运行时间，用`nvprof ./add_cuda`命令来显示内核运行时间
7. 现在已经运行了一个内核，其中一个线程可以进行一个计算，但是还没有并行，需要设置`<<<1,1>>>`语法，成为执行配置，显示cuda运行时有多少并行线程用于GPU上的启动。这里有两个参数，第二个参数：线程块中的线程数。CUDA GPU使用大小为32的线程块运行内核，因此256线程是合理的大小
8. 仅做以上修改还是为每个线程执行一个运算，而不是跨并行线程传播计算。要正确运行，我需要修改内核。CUDA C++提供的关键字让内核获得正在运行的线程的索引。具体来说，`threadIdx.x`包含其块内当前线程的索引，`blockDim.x`包含块中的线程数。修改循环以使用并行线程：

```
__global__
void add(int n, float *x, float *y)
{
  int index = threadIdx.x;
  int stride = blockDim.x;
  for (int i = index; i < n; i += stride)
      y[i] = x[i] + y[i];
}
```

9. 再用nvprof来编译，可以看到时间从157ms加快到12ms。
10. CUDA GPU有许多并行处理器，称为streaming Multiprocessors，或SMs。每个SM可以运行多个并发线程块。为了充分利用这些线程，我应该使用多个线程块启动内核。

```
int blockSize = 256;
int numBlocks = (N + blockSize - 1) / blockSize;
add<<<numBlocks, blockSize>>>(N, x, y);
```
![](https://devblogs.nvidia.com/wp-content/uploads/2017/01/cuda_indexing.png)

gridDim.x（线程块数）
blockDim.x（每个块中的线程数）
blockIdx.x（当前块内的索引）
threadIdx.x（块中当前线程的索引）

```python
__global__
void add(int n, float *x, float *y)
{
  int index = blockIdx.x * blockDim.x + threadIdx.x;
  int stride = blockDim.x * gridDim.x;
  for (int i = index; i < n; i += stride)
    y[i] = x[i] + y[i];
}
```


# 《GPU高性能编程 CUDA实战》[笔记](https://blog.csdn.net/fishseeker/article/details/75093166)






学习教程[CSDN/卜居](https://blog.csdn.net/kkk584520/article/details/9413973)

cuda编译器nvcc




0
