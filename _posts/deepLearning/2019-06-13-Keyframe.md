---
layout: post
title: 关键帧提取
tags:
categories: deepLearning
description: 如何提取视频当中的关键帧，有哪些常用算法？传统算法和基于机器学习的算法
---

# 关键帧提取的应用场景

视频检索领域中视频数据存在数据量大、 维度高的特点，在检索过程中需要消耗大量的内存与搜索时间。

随着移动互联网的普及，对视频片段的搜索也具有重要的意义，例如： 根据某个视频片段检索出数据库中相似的视频集，该技术可作为许多应用的预处理步骤。

视频包含了空间域、时间域、剧情等信息，直接对视频进行特征提取与索引是极为复杂的工作，并且需要消耗大量的存储空间与计算时间。

大多数的方案对镜头每帧进行 **相似性的度量**，并与某个固定的阈值进行比较来选出关键帧[12]，然而该阈值确实难以确定，并且难以适合每个镜头。

首先计算镜头中所有帧的平均值，将平均值与向量平均值最为接近的帧作为关键帧，由此实现了自适应的关键帧提取。

在提取视频每个镜头关键帧之后，通过对视频帧进行基于内容的检索方案即可高效、准确地检索出相关的视频序列。

参考[梁建胜,温贺平,基于深度学习的视频关键帧提取与视频检索]

# UQ_VIDEO数据集

宋景宽，易阳，黄子，沉恒涛，Richang Hong：实时大规模近似重复视频检索的多重特征哈希。ACM Multimedia，第423-432页，2011年。

http://staff.itee.uq.edu.au/shenht/UQ_VIDEO/

名为UQ_VIDEO的Combined Dataset 是我们自己创建的视频数据集，它将CC_WEB_VIDEO添加到我们从YouTube下载的视频中。

我们选择最受欢迎的400个查询来查询来自YouTube的视频。这些查询是从Google Zeitgeist中选择的。每年，Google都会检查全球各地用户在Google搜索中输入的数十亿条查询，以发现Google Zeitgeist Archives中保存的Zeitgeist。我们从2004年到2009年收集了Google Zeitgeist Archives，并选择了最受欢迎的400个查询来搜索YouTube。每个查询的下载视频数量最多为1000个。我们从2010年7月到2010年9月抓取了超过20万个YouTube视频。

过滤掉尺寸大于10M的视频后，合并数据集总共包含169,952个视频。据我们所知，这是用于实验目的的最大的Web视频数据集。我们从这些视频中进一步提取了3,305,525个关键帧。该数据集向公众发布，以便其他研究人员能够将其用作试验台。

# UQ_IMH数据集

宋景宽，杨洋，杨毅，黄子，沉恒涛：从异质数据源大规模检索的跨媒体哈希。ACM SIGMOD，第785-796页，2013年。

http://staff.itee.uq.edu.au/shenht/UQ_IMH/index.htm

在SIGMOD 2013中标题为“从异构数据源进行大规模检索的媒体间哈希”的工作中，使用了两个图像数据集和一个文本集合进行评估。图像数据集包括ImageNet语料库和NUS_WIDE图像集。对于文本数据集，我们从Web收集了一组Web文档。

1. NUS-WIDE是一个Web图像数据集，包含从Flickr下载的269,648个图像。为评估提供了81个语义概念的真实标记。删除没有标签的图像后，我们剩下267,465张图像。我们随机选择10,000张图像及其来自数据集的标签作为训练数据，以用于桥接以下图像数据集和文本文档数据集。剩下的257,465幅图像作为小图像数据集，用于实验中的测试。

2. ImageNet是根据WordNet层次结构组织的非常大的图像数据库，其中层次结构的每个节点由一组图像描绘。目前，他们每个节点平均有超过500个图像。我们使用NUS-WIDE数据集中提供的5,018个标签作为搜索ImageNet的关键字，最后得到2,904个同义词（或概念）。通过使用这些同义词作为查询，我们从ImageNet收集了大约240万个图像。随机选择来自每个synset的3个图像作为训练图像数据，并将其余图像作为用于测试的大图像数据集。
共有2,413,987张图片从imageNet下载。

3.使用Google搜索引擎抓取Web文档。从ImageNet获得的2,904个概念用作文本查询。对于每个查询，我们都会下载Google返回的前100个网页。然后，我们从每个查询的结果中随机选择3个Web文档作为训练文本数据，其余用于测试。
使用谷歌收集了240,903份文件。

# YouTube-8M 数据集

用的最多

https://blog.csdn.net/u010167269/article/details/52740990

https://research.google.com/youtube8m/

# github

https://github.com/AllenAnthony/Key-Frame

# ffmpeg

https://blog.csdn.net/qingyuanluofeng/article/details/45375647

能够读取视频，并生成图片，这其中当然有对关键帧的提取

# python实现视频关键帧提取（基于帧间差分）
https://blog.csdn.net/u011583927/article/details/84842915

算法的原理很简单：我们知道，将两帧图像进行差分，得到图像的平均像素强度可以用来衡量两帧图像的变化大小。因此，基于帧间差分的平均强度，每当视频中的某一帧与前一帧画面内容产生了大的变化，我们便认为它是关键帧，并将其提取出来。

# 帧间差分法、背景减法、光流场法
https://blog.csdn.net/zhang1308299607/article/details/80081553

## 帧间差分法

摄像机采集的视频序列具有连续性的特点。如果场景内没有运动目标，则连续帧的变化很微弱，如果存在运动目标，则连续的帧和帧之间会有明显地变化。

该类算法对时间上连续的两帧或三帧图像进行差分运算，不同帧对应的像素点相减，判断灰度差的绝对值，当绝对值超过一定阈值时，即可判断为运动目标，从而实现目标的检测功能。

两帧差分法适用于目标运动较为缓慢的场景，当运动较快时，由于目标在相邻帧图像上的位置相差较大，两帧图像相减后并不能得到完整的运动目标，因此，人们在两帧差分法的基础上提出了三帧差分法。

帧间差分法的原理简单，计算量小，能够快速检测出场景中的运动目标。但由实验结果可以看出，帧间差分法检测的目标不完整，内部含有“空洞”，这是因为运动目标在相邻帧之间的位置变化缓慢，目标内部在不同帧图像中相重叠的部分很难检测出来。帧间差分法通常不单独用在目标检测中，往往与其它的检测算法结合使用。

## 背景减弱法

对于一个稳定的监控场景而言，在没有运动目标，光照没有变化的情况下，视频图像中各个像素点的灰度值是符合随机概率分布的。由于摄像机在采集图像的过程中，会不可避免地引入噪声，这些灰度值以某一个均值为基准线，在附近做一定范围内的随机振荡，这种场景就是所谓的“背景”。

背景减法(Background subtraction)是当前运动目标检测技术中应用较为广泛的一类方法，它的基本思想和帧间差分法相类似，都是利用不同图像的差分运算提取目标区域。不过与帧间差分法不同的是，背景减法不是将当前帧图像与相邻帧图像相减，而是将当前帧图像与一个不断更新的背景模型相减，在差分图像中提取运动目标。

## 光流场法
利用光流场法实现目标检测的基本思想是：首先计算图像中每一个像素点的运动向量，即建立整幅图像的光流场。如果场景中没有运动目标，则图像中所有像素点的运动向量应该是连续变化的；如果有运动目标，由于目标和背景之间存在相对运动，目标所在位置处的运动向量必然和邻域(背景)的运动向量不同，从而检测出运动目标。

通过计算光流场得到的像素运动向量是由目标和摄像机之间的相对运动产生的。因此该类检测方法可以适用于摄像机静止和运动两种场合。但是光流场的计算过于复杂，而且在实际情况中， 由于光线等因素的影响，目标在运动时，其表面的亮度并不是保持不变的，这就不满足光流基本约束方程的假设前提，导致计算会出现很大的误差。光流场法很少应用于实际的

## 帧间差分法的opencv实现

https://blog.csdn.net/baidu_38172402/article/details/81842319

https://blog.csdn.net/u011583927/article/details/84842915

# 几种关键帧抽取算法

https://wenku.baidu.com/view/598512156bec0975f565e24d.html

* 抽样
* 镜头边界
* 颜色特征
* 运动分析
* 聚类

基于聚类实现
https://blog.csdn.net/ZJU_fish1996/article/details/54135837
