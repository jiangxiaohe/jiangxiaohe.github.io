---
layout: post
title: cs231n计算机视觉
tags:
categories: deepLearning
description: 课程笔记
---

* [官方笔记](http://cs231n.github.io/)
* [官方笔记翻译版](https://zhuanlan.zhihu.com/p/21930884)

# 数据驱动方法
给计算机很多数据，然后实现学习算法，让计算机学习到每个类的外形。这种方法，就是数据驱动方法。

* 用于超参数调优的验证集
* 交叉验证。有时候，训练集数量较小（因此验证集的数量更小），人们会使用一种被称为交叉验证的方法，这种方法更加复杂些。还是用刚才的例子，如果是交叉验证集，我们就不是取1000个图像，而是将训练集平均分成5份，其中4份用来训练，1份用来验证。然后我们循环着取其中4份来训练，其中1份来验证，最后取所有5次验证结果的平均值作为算法验证结果。
* 在实际情况下，人们不是很喜欢用交叉验证，主要是因为它会耗费较多的计算资源。一般直接把训练集按照50%-90%的比例分成训练集和验证集。但这也是根据具体情况来定的：如果超参数数量多，你可能就想用更大的验证集，而验证集的数量不够，那么最好还是用交叉验证吧。至于分成几份比较好，一般都是分成3、5和10份。
* 常用的数据分割模式。给出训练集和测试集后，训练集一般会被均分。这里是分成5份。前面4份用来训练，最后一份用作验证集调优。如果采取交叉验证，那就各份轮流作为验证集。最后模型训练完毕，超参数都定好了，让模型跑一次（而且只跑一次）测试集，以此测试结果评价算法。

# KNN
L1: $d_{1}\left(I_{1}, I_{2}\right)=\sum_{p}\left|I_{1}^{p}-I_{2}^{p}\right|$

```python
distances = np.sum(np.abs(self.Xtr - X[i,:]), axis = 1)
min_index = np.argmin(distances) # get the index with smallest distance
Ypred[i] = self.ytr[min_index] # predict the label of the nearest example
```

L2: $d_{2}\left(I_{1}, I_{2}\right)=\sqrt{\sum_{p}\left(I_{1}^{p}-I_{2}^{p}\right)^{2}}$

```python
distances = np.sqrt(np.sum(np.square(self.Xtr - X[i,:]), axis = 1))
```

# 线性分类
* 评分函数（score function）：它是原始图像数据到类别分值的映射。
* 损失函数（loss function）：它是用来量化预测分类标签的得分与真实标签之间一致性的。
* 线性分类器： $f\left(x_{i}, W, b\right)=W x_{i}+b$
* 理解线性分类器：将图像看做高维空间的点，将每个线性分类器看做高维空间的维度少一的分类平面。
* 另一种理解方式是将线性分类器看做模板匹配：关于权重W的另一个解释是它的每一行对应着一个分类的模板（有时候也叫作原型）。一张图像对应不同分类的得分，是通过使用内积（也叫点积）来比较图像和模板，然后找到和哪个模板最相似。从这个角度来看，线性分类器就是在利用学习到的模板，针对图像做模板匹配。从另一个角度来看，可以认为还是在高效地使用k-NN，不同的是我们没有使用所有的训练集的图像来比较，而是每个类别只用了一张图片（这张图片是我们学习到的，而不是训练集中的某一张），而且我们会使用（负）内积来计算向量间的距离，而不是使用L1或者L2距离。
* 偏差和权重的合并技巧: 分开处理这两个参数（权重参数W和偏差参数b）有点笨拙，一般常用的方法是把两个参数放到同一个矩阵中，同时xi向量就要增加一个维度，这个维度的数值是常量1，这就是默认的偏差维度。这样新的公式就简化成下面这样： $f\left(x_{i}, W \right)=W x_{i}$
* 图像预处理：
    * 去中心化：在这些图片的例子中，该步骤意味着根据训练集中所有的图像计算出一个平均图像值，然后每个图像都减去这个平均值，这样图像的像素值就大约分布在[-127, 127]之间了。
    * 归一化：让所有数值分布的区间变为[-1, 1]。
* 常见的线性分类函数有SVM和softmax。

# 多类支持向量机（SVM）
* 评分函数： $f\left(x_{i}, W \right)=W x_{i}$ 。这里我们将分值简写为s。比如，针对第j个类别的得分就是第j个元素：$s_{j}=f\left(x_{i}, W\right)_ {j}$
* 损失函数： 针对第i个数据的多类SVM的损失函数定义为： $L_{i}=\sum_{j \neq y_{i}} \max \left(0, s_{j}-s_{y_{i}}+\Delta\right)$
    * SVM的损失函数想要SVM在正确分类上的得分始终比不正确分类上的得分高出一个边界值 $\Delta$
    * 这里关于0的阈值被称为折叶损失（hinge loss），即max(0,-)函数。
* 正则化：可能有很多个W都可以正确的分类全部数据。
    * 一个简单的例子：如果W能够正确分类所有数据，即对于每个数据，损失值都是0。那么当 $\lambda>1$ 时，任何数乘 $\lambda W$ 都能使得损失值为0，因为这个变化将所有分值的大小都均等地扩大了，所以它们之间的绝对差值也扩大了。举个例子，如果一个正确分类的分值和举例它最近的错误分类的分值的差距是15，对W乘以2将使得差距变成30。换句话说，我们希望能向某些特定的权重W添加一些偏好，对其他权重则不添加，以此来消除模糊性。这一点是能够实现的，方法是向损失函数增加一个正则化惩罚（regularization penalty）$R(W)$ 部分。
    * 最常用的正则化惩罚是L2范式，L2范式通过对所有参数进行逐元素的平方惩罚来抑制大数值的权重： $R(W)=\sum_{k} \sum_{l} W_{k, l}^{2}$
* SVM完整的损失函数包括数据损失（data loss），即所有样例的的平均损失 $L_i$ ，以及正则化损失（regularization loss）。完整公式如下所示：
$$
L=\underbrace{\frac{1}{N} \sum_{i} L_{i}}_{\text { data loss }}+\underbrace{\lambda R(W)}_{\text { reguarization loss }}
$$

* 将其展开的完整公式为：
$$
L=\frac{1}{N} \sum_{i} \sum_{j \neq y_{i}}\left[\max \left(0, f\left(x_{i} ; W\right)_ {j}-f\left(x_{i} ; W\right)_ {y_{i}}+\Delta\right)\right]+\lambda \sum_{k} \sum_{l} W_{k, l}^{2}
$$

```python
# 非向量化
def svm_loss_naive(W, X, y, reg):
# 向量化
def svm_loss_vectorized(W, X, y, reg):
```

# Softmax分类器
* 和SVM的折叶损失不同的是，这里用的是交叉熵损失： $L i=-\log \left(\frac{e^{f_{y_{i}}}}{\sum_{j} e^{f_{j}}}\right)$ 或等价的 $L_{i}=-f_{y_{i}}+\log \left(\sum_{j} e^{f_{j}}\right)$
* 函数 $f_{j}(z)=\frac{e^{z_{j}}}{\sum_{k} e^{z_{k}}}$ 被称为softmax函数：其输入值是一个向量，向量中元素为任意实数的评分值（z中的），函数对其进行压缩，输出一个向量，其中每个元素值在0到1之间，且所有元素之和为1。
* 使数值稳定的操作： 因为损失函数的中间项 $e^{z_{j}}$ 可能数值很大，除以大数值可能会导致数值计算的不稳定，所以采用归一化技巧：
$$
\frac{e^{f_{y_{i}}}}{\sum_{j} e^{f_{j}}}=\frac{C e^{f_{y_{i}}}}{C \sum_{j} e^{f_{j}}}=\frac{e^{f_{y_{i}}+\log C}}{\sum_{j} e^{f_{j}+\log C}}
$$
* 就是应该将向量f中的数值进行平移，使得最大值为0。

```python
f = np.array([123, 456, 789]) # 例子中有3个分类，每个评分的数值都很大
p = np.exp(f) / np.sum(np.exp(f)) # 不妙：数值问题，可能导致数值爆炸
# 那么将f中的值平移到最大值为0：
f -= np.max(f) # f becomes [-666, -333, 0]
p = np.exp(f) / np.sum(np.exp(f)) # 现在OK了，将给出正确结果
```
