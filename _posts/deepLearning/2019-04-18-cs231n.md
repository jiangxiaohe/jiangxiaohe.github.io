---
layout: post
title: cs231n计算机视觉
tags:
categories: deepLearning
description: 课程笔记
---

* [官方笔记](http://cs231n.github.io/)
* [官方笔记翻译版](https://zhuanlan.zhihu.com/p/21930884)

# 数据驱动方法
给计算机很多数据，然后实现学习算法，让计算机学习到每个类的外形。这种方法，就是数据驱动方法。

* 用于超参数调优的验证集
* 交叉验证。有时候，训练集数量较小（因此验证集的数量更小），人们会使用一种被称为交叉验证的方法，这种方法更加复杂些。还是用刚才的例子，如果是交叉验证集，我们就不是取1000个图像，而是将训练集平均分成5份，其中4份用来训练，1份用来验证。然后我们循环着取其中4份来训练，其中1份来验证，最后取所有5次验证结果的平均值作为算法验证结果。
* 在实际情况下，人们不是很喜欢用交叉验证，主要是因为它会耗费较多的计算资源。一般直接把训练集按照50%-90%的比例分成训练集和验证集。但这也是根据具体情况来定的：如果超参数数量多，你可能就想用更大的验证集，而验证集的数量不够，那么最好还是用交叉验证吧。至于分成几份比较好，一般都是分成3、5和10份。
* 常用的数据分割模式。给出训练集和测试集后，训练集一般会被均分。这里是分成5份。前面4份用来训练，最后一份用作验证集调优。如果采取交叉验证，那就各份轮流作为验证集。最后模型训练完毕，超参数都定好了，让模型跑一次（而且只跑一次）测试集，以此测试结果评价算法。

# KNN
L1: $d_{1}\left(I_{1}, I_{2}\right)=\sum_{p}\left|I_{1}^{p}-I_{2}^{p}\right|$

```python
distances = np.sum(np.abs(self.Xtr - X[i,:]), axis = 1)
min_index = np.argmin(distances) # get the index with smallest distance
Ypred[i] = self.ytr[min_index] # predict the label of the nearest example
```

L2: $d_{2}\left(I_{1}, I_{2}\right)=\sqrt{\sum_{p}\left(I_{1}^{p}-I_{2}^{p}\right)^{2}}$

```python
distances = np.sqrt(np.sum(np.square(self.Xtr - X[i,:]), axis = 1))
```

```python
# compute_distances_two_loops
for i in range(num_test):
    for j in range(num_train):
        dists[i][j] = np.sqrt(np.sum(np.square(X[i,:] - self.X_train[j,:])))

# compute_distances_one_loop
for i in range(num_test):
    dists[i, :] = np.sqrt(np.sum(np.square(self.X_train - X[i, :]),axis=1))

# compute_distances_no_loops
dists = np.multiply(np.dot(X,self.X_train.T),-2)
sq1 = np.sum(np.square(X),axis=1,keepdims = True)
sq2 = np.sum(np.square(self.X_train),axis=1)
dists = np.add(dists,sq1)
dists = np.add(dists,sq2)
dists = np.sqrt(dists)

# predict_labels
closest_y = self.y_train[np.argsort(dists[i])[0:k]]
y_pred[i] = np.bincount(closest_y).argmax()
```


# 线性分类
* 评分函数（score function）：它是原始图像数据到类别分值的映射。
* 损失函数（loss function）：它是用来量化预测分类标签的得分与真实标签之间一致性的。
* 线性分类器： $f\left(x_{i}, W, b\right)=W x_{i}+b$
* 理解线性分类器：将图像看做高维空间的点，将每个线性分类器看做高维空间的维度少一的分类平面。
* 另一种理解方式是将线性分类器看做模板匹配：关于权重W的另一个解释是它的每一行对应着一个分类的模板（有时候也叫作原型）。一张图像对应不同分类的得分，是通过使用内积（也叫点积）来比较图像和模板，然后找到和哪个模板最相似。从这个角度来看，线性分类器就是在利用学习到的模板，针对图像做模板匹配。从另一个角度来看，可以认为还是在高效地使用k-NN，不同的是我们没有使用所有的训练集的图像来比较，而是每个类别只用了一张图片（这张图片是我们学习到的，而不是训练集中的某一张），而且我们会使用（负）内积来计算向量间的距离，而不是使用L1或者L2距离。
* 偏差和权重的合并技巧: 分开处理这两个参数（权重参数W和偏差参数b）有点笨拙，一般常用的方法是把两个参数放到同一个矩阵中，同时xi向量就要增加一个维度，这个维度的数值是常量1，这就是默认的偏差维度。这样新的公式就简化成下面这样： $f\left(x_{i}, W \right)=W x_{i}$
* 图像预处理：
    * 去中心化：在这些图片的例子中，该步骤意味着根据训练集中所有的图像计算出一个平均图像值，然后每个图像都减去这个平均值，这样图像的像素值就大约分布在[-127, 127]之间了。
    * 归一化：让所有数值分布的区间变为[-1, 1]。
* 常见的线性分类函数有SVM和softmax。

# 多类支持向量机（SVM）
* 评分函数： $f\left(x_{i}, W \right)=W x_{i}$ 。这里我们将分值简写为s。比如，针对第j个类别的得分就是第j个元素：$s_{j}=f\left(x_{i}, W\right)_ {j}$
* 损失函数： 针对第i个数据的多类SVM的损失函数定义为： $L_{i}=\sum_{j \neq y_{i}} \max \left(0, s_{j}-s_{y_{i}}+\Delta\right)$
    * SVM的损失函数想要SVM在正确分类上的得分始终比不正确分类上的得分高出一个边界值 $\Delta$
    * 这里关于0的阈值被称为折叶损失（hinge loss），即max(0,-)函数。
* 正则化：可能有很多个W都可以正确的分类全部数据。
    * 一个简单的例子：如果W能够正确分类所有数据，即对于每个数据，损失值都是0。那么当 $\lambda>1$ 时，任何数乘 $\lambda W$ 都能使得损失值为0，因为这个变化将所有分值的大小都均等地扩大了，所以它们之间的绝对差值也扩大了。举个例子，如果一个正确分类的分值和举例它最近的错误分类的分值的差距是15，对W乘以2将使得差距变成30。换句话说，我们希望能向某些特定的权重W添加一些偏好，对其他权重则不添加，以此来消除模糊性。这一点是能够实现的，方法是向损失函数增加一个正则化惩罚（regularization penalty）$R(W)$ 部分。
    * 最常用的正则化惩罚是L2范式，L2范式通过对所有参数进行逐元素的平方惩罚来抑制大数值的权重： $R(W)=\sum_{k} \sum_{l} W_{k, l}^{2}$
* SVM完整的损失函数包括数据损失（data loss），即所有样例的的平均损失 $L_i$ ，以及正则化损失（regularization loss）。完整公式如下所示：
$$
L=\underbrace{\frac{1}{N} \sum_{i} L_{i}}_{\text { data loss }}+\underbrace{\lambda R(W)}_{\text { reguarization loss }}
$$

* 将其展开的完整公式为：
$$
L=\frac{1}{N} \sum_{i} \sum_{j \neq y_{i}}\left[\max \left(0, f\left(x_{i} ; W\right)_ {j}-f\left(x_{i} ; W\right)_ {y_{i}}+\Delta\right)\right]+\lambda \sum_{k} \sum_{l} W_{k, l}^{2}
$$

```python
"""
Inputs:
- W: A numpy array of shape (D, C) containing weights.
- X: A numpy array of shape (N, D) containing a minibatch of data.
- y: A n.。umpy array of shape (N,) containing training labels; y[i] = c means
  that X[i] has label c, where 0 <= c < C.
- reg: (float) regularization strength

Returns a tuple of:
- loss as single float
- gradient with respect to weights W; an array of same shape as W
"""
# 非向量化
def svm_loss_naive(W, X, y, reg):
    dW = np.zeros(W.shape) # initialize the gradient as zero
    # compute the loss and the gradient
    num_classes = W.shape[1]
    num_train = X.shape[0]
    loss = 0.0
    for i in range(num_train):
        scores = X[i].dot(W)
        correct_class_score = scores[y[i]]
        for j in range(num_classes):
            if j == y[i]:
                continue
            margin = scores[j] - correct_class_score + 1 # note delta = 1
            if margin > 0:
                loss += margin
                dW[:,y[i]] += -X[i,:].T
                dW[:,j] += X[i,:].T
    loss /= num_train
    dW /= num_train
    loss += 0.5 * reg * np.sum(W * W)
    dW += reg * W
    return loss, dW

# 向量化
def svm_loss_vectorized(W, X, y, reg):
    loss = 0.0
    dW = np.zeros(W.shape)
    scores = X.dot(W) # N x C
    num_train = X.shape[0]
    margin = scores - scores[range(0,num_train), y].reshape(-1, 1) + 1 # N x C
    margin[range(num_train), y] = 0
    margins = np.maximum(0, margins)
    loss += np.sum(margins) / num_train
    loss += 0.5 * reg * np.sum( W * W)

    margins[margins>0]=1.0
    row_sum = np.sum(margins,axis=1)
    margins[np.arange(num_train),y] = -row_sum
    dW = 1.0/num_train*np.dot(X.T,margins) + reg*W

    return loss, dW
```

# Softmax分类器
* 和SVM的折叶损失不同的是，这里用的是交叉熵损失： $L i=-\log \left(\frac{e^{f_{y_{i}}}}{\sum_{j} e^{f_{j}}}\right)$ 或等价的 $L_{i}=-f_{y_{i}}+\log \left(\sum_{j} e^{f_{j}}\right)$
* 函数 $f_{j}(z)=\frac{e^{z_{j}}}{\sum_{k} e^{z_{k}}}$ 被称为softmax函数：其输入值是一个向量，向量中元素为任意实数的评分值（z中的），函数对其进行压缩，输出一个向量，其中每个元素值在0到1之间，且所有元素之和为1。
* 使数值稳定的操作： 因为损失函数的中间项 $e^{z_{j}}$ 可能数值很大，除以大数值可能会导致数值计算的不稳定，所以采用归一化技巧：
$$
\frac{e^{f_{y_{i}}}}{\sum_{j} e^{f_{j}}}=\frac{C e^{f_{y_{i}}}}{C \sum_{j} e^{f_{j}}}=\frac{e^{f_{y_{i}}+\log C}}{\sum_{j} e^{f_{j}+\log C}}
$$
* 就是应该将向量f中的数值进行平移，使得最大值为0。

```python
f = np.array([123, 456, 789]) # 例子中有3个分类，每个评分的数值都很大
p = np.exp(f) / np.sum(np.exp(f)) # 不妙：数值问题，可能导致数值爆炸
# 那么将f中的值平移到最大值为0：
f -= np.max(f) # f becomes [-666, -333, 0]
p = np.exp(f) / np.sum(np.exp(f)) # 现在OK了，将给出正确结果
```

```python
def softmax_loss_naive(W, X, y, reg):
    loss = 0.0
    dW = np.zeros_like(W)

    # compute the loss and the gradient
    num_classes = W.shape[1]
    num_train = X.shape[0]
    loss = 0.0

    for i in range(num_train):
        scores = X[i].dot(W)
        scores -= np.max(scores)
        scores = np.exp(scores)
        scores_sum = np.sum(scores)
        p = scores[y[i]] / scores_sum
        loss += -np.log(p)

        for j in range(num_classes):
            dW[:,j] += X[i,:].T * scores[j] / scores_sum
            if j == y[i]:
                dW[:,j] += -X[i,:].T

    loss /= num_train
    dW /= num_train
    loss += 0.5 * reg * np.sum(W * W)
    dW += reg * W   

    return loss, dW

def softmax_loss_vectorized(W, X, y, reg):

    dW = np.zeros_like(W)   
    num_classes = W.shape[1]
    num_train = X.shape[0]
    loss = 0.0

    scores = X.dot(W)
    scores -= scores.max(axis = 1).reshape(num_train, 1)
    scores_sum = np.exp(scores).sum(axis = 1)
    loss += np.log(scores_sum).sum() - scores[range(num_train), y].sum()

    scores = np.true_divide(scores,scores_sum)
    scores[range(num_train), y] -= 1
    dW = np.dot(X.T, scores)

    loss = loss / num_train + 0.5 * reg * np.sum(W * W)
    dW = dW / num_train + reg * W
```

# 反向传播

搞清楚链式法则和计算线路图像。

```python
# 设置输入值
x = -2; y = 5; z = -4

# 进行前向传播
q = x + y # q becomes 3
f = q * z # f becomes -12

# 进行反向传播:
# 首先回传到 f = q * z
dfdz = q # df/dz = q, 所以关于z的梯度是3
dfdq = z # df/dq = z, 所以关于q的梯度是-4
# 现在回传到q = x + y
dfdx = 1.0 * dfdq # dq/dx = 1. 这里的乘法是因为链式法则
dfdy = 1.0 * dfdq # dq/dy = 1
```

* 模块化：Sigmoid例子

$$
f(w, x)=\frac{1}{1+e^{-\left(w_{0} x_{0}+w_{1} x_{1}+w_{2}\right)}}
$$

$$
\sigma(x)=\frac{1}{1+e^{-x}}
\rightarrow \frac{d \sigma(x)}{d x}=\frac{e^{-x}}{\left(1+e^{-x}\right)^{2}}=\left(\frac{1+e^{-x}-1}{1+e^{-x}}\right)\left(\frac{1}{1+e^{-x}}\right)=(1-\sigma(x)) \sigma(x)
$$

* 分段计算实例：

$$
f(x, y)=\frac{x+\sigma(y)}{\sigma(x)+(x+y)^{2}}
$$

首先要说的是，这个函数完全没用，读者是不会用到它来进行梯度计算的，这里只是用来作为实践反向传播的一个例子，需要强调的是，如果对x或y进行微分运算，运算结束后会得到一个巨大而复杂的表达式。然而做如此复杂的运算实际上并无必要，因为我们不需要一个明确的函数来计算梯度，只需知道如何使用反向传播计算梯度即可。下面是构建前向传播的代码模式：

```python
x = 3 # 例子数值
y = -4

# 前向传播
sigy = 1.0 / (1 + math.exp(-y)) # 分子中的sigmoi          #(1)
num = x + sigy # 分子                                    #(2)
sigx = 1.0 / (1 + math.exp(-x)) # 分母中的sigmoid         #(3)
xpy = x + y                                              #(4)
xpysqr = xpy**2                                          #(5)
den = sigx + xpysqr # 分母                                #(6)
invden = 1.0 / den                                       #(7)
f = num * invden # 搞定！                                 #(8)

# 回传 f = num * invden
dnum = invden # 分子的梯度                                         #(8)
dinvden = num                                                     #(8)
# 回传 invden = 1.0 / den
dden = (-1.0 / (den**2)) * dinvden                                #(7)
# 回传 den = sigx + xpysqr
dsigx = (1) * dden                                                #(6)
dxpysqr = (1) * dden                                              #(6)
# 回传 xpysqr = xpy**2
dxpy = (2 * xpy) * dxpysqr                                        #(5)
# 回传 xpy = x + y
dx = (1) * dxpy                                                   #(4)
dy = (1) * dxpy                                                   #(4)
# 回传 sigx = 1.0 / (1 + math.exp(-x))
dx += ((1 - sigx) * sigx) * dsigx # Notice += !! See notes below  #(3)
# 回传 num = x + sigy
dx += (1) * dnum                                                  #(2)
dsigy = (1) * dnum                                                #(2)
# 回传 sigy = 1.0 / (1 + math.exp(-y))
dy += ((1 - sigy) * sigy) * dsigy                                 #(1)
```

# 神经网络

一个三层神经网络的前向传播举例：

```python
f = lambda x: 1.0/(1.0 + np.exp(-x)) # 激活函数(用的sigmoid)
x = np.random.randn(3, 1) # 含3个数字的随机输入向量(3x1)
h1 = f(np.dot(W1, x) + b1) # 计算第一个隐层的激活数据(4x1)
h2 = f(np.dot(W2, h1) + b2) # 计算第二个隐层的激活数据(4x1)
out = np.dot(W3, h2) + b3 # 神经元输出(1x1)
```

## 损失函数
## 梯度

0
