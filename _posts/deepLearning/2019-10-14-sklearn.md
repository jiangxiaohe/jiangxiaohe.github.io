---
layout: post
title: sklearn库学习
tags:
- deepLearning
categories: deepLearning
description: 学习sklearn中的各个函数
---

sklearn是简单高效的数据挖掘和数据分析工具，conda下的安装方法为`conda install scikit-learn`。
升级和卸载用conda的update和remove命令。

[中文文档地址](https://sklearn.apachecn.org/docs/0.21.3/)

在Sklearn当中有三大模型：Transformer 转换器、Estimator 估计器、Pipeline 管道

所有的模型都继承自类[Estimator](https://github.com/scikit-learn/scikit-learn/blob/1495f69242646d239d89a5713982946b8ffcf9d9/sklearn/base.py)

该类的方法有：
* get_params(self, deep=True)获取模型的设置参数，初始化时输入的参数
* set_params(self, **params) 设置参数，输入参数为字典
* fit(X, Y)
* score(X)
* predict(X)


[项目举例](https://github.com/scikit-learn-contrib/project-template/)


# 1. 监督学习

## 1.1. 广义线性模型

class sklearn.linear_model.LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=None)

LinearRegression 拟合一个带有系数 w = (w_1, ..., w_p) 的线性模型，使得数据集实际观测数据和预测数据（估计值）之间的残差平方和最小。

LinearRegression ： $ \min _{w}\|X w-y\|_{2}^{2}$

Ridge ： $\min _{w}\|X w-y\|_{2}^{2}+\alpha\|w\|_{2}^{2}$
 \alpha 是控制系数收缩量的复杂性参数： \alpha 的值越大，收缩量越大，模型对共线性的鲁棒性也更强。

此外，还有：

Lasso ：
$$
\min _{w} \frac{1}{2 n_{\text {samples}}}\|X w-y\|_{2}^{2}+\alpha\|w\|_{1}
$$



## 1.2. 线性和二次判别分析
## 1.3. 内核岭回归
## 1.4. 支持向量机
## 1.5. 随机梯度下降
## 1.6. 最近邻
## 1.7. 高斯过程
## 1.8. 交叉分解
## 1.9. 朴素贝叶斯
## 1.10. 决策树

Decision Trees (DTs) 是一种用来 classification 和 regression 的无参监督学习方法。其目的是创建一种模型从数据特征中学习简单的决策规则来预测一个目标变量的值。

class sklearn.tree.DecisionTreeClassifier(criterion=’gini’, splitter=’best’, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort=False)

criterion : string, optional (default=”gini”)
The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain.



* 分类问题

```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report

X_train = [[0., 0.], [1., 1.]]
y_train = [0, 1]
clf = DecisionTreeClassifier(max_depth=10)
# 训练
clf.fit(X_train, y_train)    
# 预测
X_test = [[2., 2.], [-1., -2.]]
y_predict = clf.predict(X_test)   
y_test = [1,1]     

# 计算准确度和召回
from sklearn.metrics import accuracy_score,recall_score
acc = accuracy_score(y_test, y_predict)
recall = recall_score(y_test, y_predict)
classification_report(y_predict, y_test)
```


## 1.11. 集成方法
## 1.12. 多类和多标签算法

线性分类器，

class sklearn.linear_model.SGDClassifier(loss=’hinge’, penalty=’l2’, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, n_jobs=None, random_state=None, learning_rate=’optimal’, eta0=0.0, power_t=0.5, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, class_weight=None, warm_start=False, average=False)

* 损失函数，默认是hinge（对应模型为SVM），选择log时模型为logregression
  * loss=”hinge”: (soft-margin) 线性SVM.
  * loss=”modified_huber”: 带平滑的hinge loss.
  * loss=”log”: logistic 回归

* 通过penalty参数，可以设置对应的惩罚项。SGD支持下面的罚项：
  * penalty=”l2”: 对coef_的L2范数罚项
  * penalty=”l1”: 对coef_的L1范数罚项
  * penalty=”elasticnet”: L2和L1的convex组合; (1 - l1_ratio) * L2 + l1_ratio * L1

SGDClassifier支持多分类，它以”one-vs-all(OVA)”的方式通过结合多个二分类来完成。对于K个类中的每个类来说，一个二分类器可以通过它和其它K-1个类来进行学习得到。在测试时，我们会为每个分类器要计算置信度（例如：到超平面的有符号距离）并选择最高置信度的类。

```python
# 生成数据
from sklearn.datasets.samples_generator import make_blobs
# 产生点，n_samples样本总个数，centers聚类中心的个数，cluster_std样本方差
X, Y = make_blobs(n_samples=50, centers=2,
                      random_state=0, cluster_std=0.60)
print(X.shape,Y.shape)
plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired)
# 调用线性分类器
from sklearn.linear_model import SGDClassifier
# loss折页损失，/sum{max(0,sj−syi+Δ)}
clf = SGDClassifier(loss="hinge", alpha=0.01,
                        max_iter=200, fit_intercept=True)
clf.fit(X, Y)
# plot the line, the points, and the nearest vectors to the plane
xx = np.linspace(-1, 5, 10)
yy = np.linspace(-1, 5, 10)

X1, X2 = np.meshgrid(xx, yy)
Z = np.empty(X1.shape)
for (i, j), val in np.ndenumerate(X1):
    x1 = val
    x2 = X2[i, j]
    p = clf.decision_function(np.array([x1, x2]).reshape(1, -1))
    p2 = clf.predict(np.array([x1, x2]).reshape(1, -1))
    print(p,p2) # 每个数据都有一个预测的分数和一个预测的分类结果

    Z[i, j] = p[0]
levels = [-1.0, 0.0, 1.0]
linestyles = ['dashed', 'solid', 'dashed']
colors = 'k'

ax = plt.axes()
ax.contour(X1, X2, Z, levels, colors=colors, linestyles=linestyles)
ax.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired)

ax.axis('tight')

# 打印函数的参数
clf.get_params()

# 打印函数的权重（w，截距）
clf.coef_,clf.intercept_
```



## 1.13. 特征选择
## 1.14. 半监督学习
## 1.15. 等式回归
## 1.16. 概率校准
## 1.17. 神经网络模型（有监督）

多层感知机即全连接神经网络，两层之间可用Y=WX+B表示。
该函数API为：

class sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(100, ), activation=’relu’, solver=’adam’, alpha=0.0001, batch_size=’auto’, learning_rate=’constant’, learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10)

* hidden_layer_sizes :例如hidden_layer_sizes=(50, 50)，表示有两层隐藏层，第一层隐藏层有50个神经元，第二层也有50个神经元。 
* activation :激活函数,{‘identity’, ‘logistic’, ‘tanh’, ‘relu’}, 默认relu 
	- identity：f(x) = x 
	- logistic：其实就是sigmod,f(x) = 1 / (1 + exp(-x)). 
	- tanh：f(x) = tanh(x). 
	- relu：f(x) = max(0, x) 
* solver： {‘lbfgs’, ‘sgd’, ‘adam’}, 默认adam，用来优化权重 
	- lbfgs：quasi-Newton方法的优化器 
	- sgd：随机梯度下降 
	- adam： Kingma, Diederik, and Jimmy Ba提出的机遇随机梯度的优化器 
* learning_rate :学习率,用于权重更新,只有当solver为’sgd’时使用，{‘constant’，’invscaling’, ‘adaptive’},默认constant 
	- ‘constant’: 有’learning_rate_init’给定的恒定学习率 
	- ‘incscaling’：随着时间t使用’power_t’的逆标度指数不断降低学习率learning_rate_ ，effective_learning_rate = learning_rate_init / pow(t, power_t)
	- ‘adaptive’：只要训练损耗在下降，就保持学习率为’learning_rate_init’不变，当连续两次不能降低训练损耗或验证分数停止升高至少tol时，将当前学习率除以5. 
* shuffle: bool，可选，默认True,只有当solver=’sgd’或者‘adam’时使用，判断是否在每次迭代时对样本进行清洗。 
* verbose : bool, 可选, 默认False,是否将过程打印到stdout 
* momentum : float, 默认 0.9,动量梯度下降更新，设置的范围应该0.0-1.0. 只有solver=’sgd’时使用. 

代码如下：

```python
from sklearn.neural_network import MLPClassifier
X_train = [[0., 0.], [1., 1.]]
y_train = [0, 1]
clf = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(5, 2), random_state=1)
# 训练
clf.fit(X_train, y_train)    
# 预测
X_test = [[2., 2.], [-1., -2.]]
y_predict = clf.predict(X_test)   
y_test = [1,1]     

# 计算准确度和召回
from sklearn.metrics import accuracy_score,recall_score
acc = accuracy_score(y_test, y_predict)
recall = recall_score(y_test, y_predict)
```



注意：多层感知机对于特征的缩放是敏感的。所以它强烈建议您归一化你的数据。 例如，将输入向量 X 的每个属性放缩到到 [0, 1] 或 [-1，+1] ，或者将其标准化使它具有 0 均值和方差 1。注意，为了得到有意义的结果，您必须对测试集也应用 相同的 尺度缩放。 您可以使用 StandardScaler 进行标准化。



# 用joblib来保存和加载模型

在机器学习中我们训练模型后，需要把模型保存到本地，这里我们采用joblib来保存。

`from sklearn.externals import joblib`

* 保存：`joblib.dump(model, filename=filepath)`
* 加载：`model = joblib.load(filepath)`

# 预处理

onehot

```python
from sklearn.preprocessing import  OneHotEncoder

enc = OneHotEncoder(sparse = False) 
ans = enc.fit_transform([[0, 0, 3],
                         [1, 1, 0],
                         [0, 2, 1],
                         [1, 0, 2]])# 这里必须是二维，可是一个1个或者多个特征
print(ans) # 输出 [[ 1.  0.  1. ...,  0.  0.  1.]
           #      [ 0.  1.  0. ...,  0.  0.  0.]
           #      [ 1.  0.  0. ...,  1.  0.  0.]
           #      [ 0.  1.  1. ...,  0.  1.  0.]]
```


