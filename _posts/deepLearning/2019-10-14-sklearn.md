---
layout: post
title: sklearn库学习
tags:
- deepLearning
categories: deepLearning
description: 学习sklearn中的各个函数
---

sklearn是简单高效的数据挖掘和数据分析工具，conda下的安装方法为`conda install scikit-learn`。
升级和卸载用conda的update和remove命令。

[中文文档地址](https://sklearn.apachecn.org/docs/0.21.3/)

# 1. 监督学习

## 1.1. 广义线性模型
## 1.2. 线性和二次判别分析
## 1.3. 内核岭回归
## 1.4. 支持向量机
## 1.5. 随机梯度下降
## 1.6. 最近邻
## 1.7. 高斯过程
## 1.8. 交叉分解
## 1.9. 朴素贝叶斯
## 1.10. 决策树
## 1.11. 集成方法
## 1.12. 多类和多标签算法
## 1.13. 特征选择
## 1.14. 半监督学习
## 1.15. 等式回归
## 1.16. 概率校准
## 1.17. 神经网络模型（有监督）

多层感知机即全连接神经网络，两层之间可用Y=WX+B表示。
该函数API为：

class sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(100, ), activation=’relu’, solver=’adam’, alpha=0.0001, batch_size=’auto’, learning_rate=’constant’, learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10)

* hidden_layer_sizes :例如hidden_layer_sizes=(50, 50)，表示有两层隐藏层，第一层隐藏层有50个神经元，第二层也有50个神经元。 
* activation :激活函数,{‘identity’, ‘logistic’, ‘tanh’, ‘relu’}, 默认relu 
	- identity：f(x) = x 
	- logistic：其实就是sigmod,f(x) = 1 / (1 + exp(-x)). 
	- tanh：f(x) = tanh(x). 
	- relu：f(x) = max(0, x) 
* solver： {‘lbfgs’, ‘sgd’, ‘adam’}, 默认adam，用来优化权重 
	- lbfgs：quasi-Newton方法的优化器 
	- sgd：随机梯度下降 
	- adam： Kingma, Diederik, and Jimmy Ba提出的机遇随机梯度的优化器 
* learning_rate :学习率,用于权重更新,只有当solver为’sgd’时使用，{‘constant’，’invscaling’, ‘adaptive’},默认constant 
	- ‘constant’: 有’learning_rate_init’给定的恒定学习率 
	- ‘incscaling’：随着时间t使用’power_t’的逆标度指数不断降低学习率learning_rate_ ，effective_learning_rate = learning_rate_init / pow(t, power_t)
	- ‘adaptive’：只要训练损耗在下降，就保持学习率为’learning_rate_init’不变，当连续两次不能降低训练损耗或验证分数停止升高至少tol时，将当前学习率除以5. 
* shuffle: bool，可选，默认True,只有当solver=’sgd’或者‘adam’时使用，判断是否在每次迭代时对样本进行清洗。 
* verbose : bool, 可选, 默认False,是否将过程打印到stdout 
* momentum : float, 默认 0.9,动量梯度下降更新，设置的范围应该0.0-1.0. 只有solver=’sgd’时使用. 

代码如下：

```python
from sklearn.neural_network import MLPClassifier
X_train = [[0., 0.], [1., 1.]]
y_train = [0, 1]
clf = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(5, 2), random_state=1)
# 训练
clf.fit(X_train, y_train)    
# 预测
X_test = [[2., 2.], [-1., -2.]]
y_predict = clf.predict(X_test)   
y_test = [1,1]     

# 计算准确度和召回
from sklearn.metrics import accuracy_score,recall_score
acc = accuracy_score(y_test, y_predict)
recall = recall_score(y_test, y_predict)
```



注意：多层感知机对于特征的缩放是敏感的。所以它强烈建议您归一化你的数据。 例如，将输入向量 X 的每个属性放缩到到 [0, 1] 或 [-1，+1] ，或者将其标准化使它具有 0 均值和方差 1。注意，为了得到有意义的结果，您必须对测试集也应用 相同的 尺度缩放。 您可以使用 StandardScaler 进行标准化。



# 用joblib来保存和加载模型

在机器学习中我们训练模型后，需要把模型保存到本地，这里我们采用joblib来保存。

`from sklearn.externals import joblib`

* 保存：`joblib.dump(model, filename=filepath)`
* 加载：`model = joblib.load(filepath)`
