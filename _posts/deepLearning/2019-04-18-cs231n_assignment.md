---
layout: post
title: cs231n计算机视觉作业
tags:
categories: deepLearning
description: 课程笔记
---

# 作业1

本作业的目标如下：

* 理解基本的图像分类流程和数据驱动方法（训练与预测阶段）。
* 理解训练、验证、测试分块，学会使用验证数据来进行超参数调优。
* 熟悉使用numpy来编写向量化代码。
* 实现并应用k-最近邻（k-NN）分类器。
* 实现并应用Softmax分类器。
* 实现并应用支持向量机（SVM）分类器。
* 实现并应用一个两层神经网络分类器。
* 理解以上分类器的差异和权衡之处。
* 基本理解使用更高层次表达相较于使用原始图像像素对算法性能的提升（例如：色彩直方图和梯度直方图HOG）。


* Q1：k-最近邻分类器（20分）knn.ipynb
* Q2：训练一个SVM（25分）svm.ipynb
* Q3：实现Softmax分类器（20分）softmax.ipynb
* Q4：实现2层神经网络（25分）two_layer_net.ipynb
* Q5：更高层次表达：图像特征（10分）features.ipynb

# 作业2

本作业的目标如下：

* 理解神经网络及其分层结构。
* 理解并实现（向量化）反向传播。
* 实现多个用于神经网络最优化的更新方法。
* 实现用于训练深度网络的批量归一化（ batch normalization ）。
* 实现随机失活（dropout）。
* 进行高效的交叉验证并为神经网络结构找到最好的超参数。
* 理解卷积神经网络的结构，并积累在数据集上训练此类模型的经验。


* Q1：全连接神经网络（30分）FullyConnectedNets.ipynb

BGD采用整个训练集的数据来计算损失函数对参数的梯度，该方法每次更新对整个数据集计算梯度，遇到大数据集会很慢。

SGD(Stochastic gradient descent) 每次更新时对每个样本进行梯度更新，对于很大的数据集来说，可能会有相似的样本，这样用BGD计算会出现冗余，而SGD一次只进行一次更新，就没有冗余。但是SGD因为更新较快，可能会造成损失函数的震荡。

MBGD(mini-batch gradient descent) 每一次利用一小批样本，这样可以降低参数更新时的方差，


[动量梯度下降](https://blog.csdn.net/yinruiyang94/article/details/77944338)

$$
\begin{array}{l}{v_{d W}=\beta v_{d W}+(1-\beta) d W} \\ {v_{d b}=\beta v_{d b}+(1-\beta) d b} \\ {W=W-\alpha v_{d W}, \quad b=b-\alpha v_{d b}}\end{array}
$$

这个就是动量梯度下降的参数更新公式。
我们可以看出，在这个公式中，并不是直接减去αdW和αdb，而是计算出了一个vdW和vdb。这又是什么呢？

在此需要引入一个叫做指数加权平均的知识点。也就是上图中的前两行公式。使用这个公式，可以将之前的dW和db都联系起来，不再是每一次梯度都是独立的情况。其中β是可以自行设置的超参数，一般情况下默认为0.9（也可以设置为其他数值）。β代表了现在的vdW和vdb与之前的1 / (1 - β)个vdW和vdb有关。0.9就是现在的vdW和vdb是平均了之前10天的vdW和vdb的结果。

此时的梯度不再只是我现在的数据的梯度，而是有一定权重的之前的梯度，就我个人认为，就像是把原本的梯度压缩一点，并且补上一个之前就已经存在的“动量”。

[ADagrad](https://blog.csdn.net/w113691/article/details/82631097)

上面提到的方法对于所有参数都使用了同一个更新速率。但是同一个更新速率不一定适合所有参数。比如有的参数可能已经到了仅需要微调的阶段，但又有些参数由于对应样本少等原因，还需要较大幅度的调动。Adagrad就是针对这一问题提出的，自适应地为各个参数分配不同学习率的算法。

* Q2：批量归一化（30分）BatchNormalization.ipynb
* Q3：随机失活（Dropout）（10分）Dropout.ipynb
* Q4：在CIFAR-10上运行卷积神经网络（30分）ConvolutionalNetworks.ipynb


# 作业3

在本作业中，你将实现循环网络，并将其应用于在微软的COCO数据库上进行图像标注。我们还会介绍TinyImageNet数据集，然后在这个数据集使用一个预训练的模型来查看图像梯度的不同应用。本作业的目标如下：

* 理解循环神经网络（RNN）的结构，知道它们是如何随时间共享权重来对序列进行操作的。
* 理解普通循环神经网络和长短基记忆（Long-Short Term Memory）循环神经网络之间的差异。
* 理解在测试时如何从RNN生成序列。
* 理解如何将卷积神经网络和循环神经网络结合在一起来实现图像标注。
* 理解一个训练过的卷积神经网络是如何用来从输入图像中计算梯度的。
* 进行高效的交叉验证并为神经网络结构找到最好的超参数。
* 实现图像梯度的不同应用，比如显著图，搞笑图像，类别可视化，特征反演和DeepDream。

* Q1：使用普通RNN进行图像标注（40分）RNN_Captioning.ipynb
* Q2：使用LSTM进行图像标注（35分）LSTM_Captioning.ipynb
* Q3：图像梯度：显著图和高效图像（10分）ImageGradients.ipynb
* Q4：图像生成：类别，反演和DeepDream（30分）ImageGeneration.ipynb




0
