---
layout: post
title: 机器学习评价指标
tags:
categories: deepLearning
description: 召回率（Recall），精确率（Precision），平均正确率（Average_precision(AP) ），mAP
---

[TOC]

# 评价指标

[YOLO学习笔记之评价指标](https://blog.csdn.net/shangpapa3/article/details/76177830)

假设现在有这样一个测试集，测试集中的图片只由大雁和飞机两种图片组成.

预测可能有四种结果：
* True positives : 飞机的图片被正确的识别成了飞机。
* True negatives: 大雁的图片没有被识别出来，系统正确地认为它们是大雁。
* False positives: 大雁的图片被错误地识别成了飞机。
* False negatives: 飞机的图片没有被识别出来，系统错误地认为它们是大雁。

positives和negatives分别代表预测是正类还是负类，true和false分别代表预测的正确性。

## Precision、Recall

$ Precision = \frac{tp}{tp+fp} $ 表示在识别为飞机的图片当中，飞机的照片所占的比例。

$ recall = \frac{tp}{tp+fn} $ 表示所有的飞机当中，被正确识别的比例。

一般来说，precision和recall负相关。通过改变阈值，会导致两者变化。如果想要评估一个分类器的性能，一个比较好的办法是：观察当阈值变化时，Precision与Recall值的变化情况。如果一个分类器的性能比较好，那么它应该有如下的表现：被识别出的图片中飞机所占的比重比较大，并且在识别出大雁之前，尽可能多地正确识别出飞机，也就是让Recall值增长的同时保持Precision的值在一个很高的水平。而性能比较差的分类器可能会损失很多Precision值才能换来Recall值的提高。通常情况下，文章中都会使用 **Precision-recall曲线**，来显示出分类器在Precision与Recall之间的权衡。

![](https://img-blog.csdn.net/20170105154145685?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvaHlzdGVyaWMzMTQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

上图就是分类器的Precision-recall 曲线，在不损失精度的条件下它能达到40%Recall。而当Recall达到100%时，Precision 降低到50%。

## AP

如果想要用一个数值而非曲线图来表示一个分类器的性能，通常用Average Precision来作为这一度量标准。

$ AP = \int_0^1{p(r)}dr $

P(r)代表正确率，r代表召回率。这个数值和P-R曲线下的面积相等。这一积分极其接近这一数值：对每一种阈值分别求Precision，乘以Recall的变化情况，再把所有阈值下求得的乘积值进行累加。

$ AP = \sum_{k=1}^N{p(k)}\Delta r(k) $

另一种度量性能的标准：Interpolated Average Precision。

$ IAP = \sum_{k=1}^N{max_{k'>=k}{P(k')}}\Delta r(k) $

![](https://img-blog.csdn.net/20170727110953087?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2hhbmdwYXBhMw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

## F-feature

最常见的方法应该是F-Measure，也称F-Score，是Precision和Recall的加权平均。

$ F = \frac{(a^2+1)P*R}{a^2(P+R)} $

当a=1时，就是最常见的F1了：

$ F1 = \frac{2PR}{P+R} $

当F1较高时则比较说明试验方法比较理想。

## [mAP](https://blog.csdn.net/u014203453/article/details/77598997)

多标签图像分类任务中图片的标签不止一个，因此评价不能用普通单标签图像分类的标准，即mean accuracy，该任务采用的是和信息检索中类似的方法—mAP（mean Average Precision），虽然其字面意思和mean accuracy看起来差不多，但是计算方法要繁琐得多，步骤如下：
1. 保存所有样本的confidence score
2. 对confidence score进行排序
3. 计算precision和recall
4. 计算AP

> 待具体搞懂！！
