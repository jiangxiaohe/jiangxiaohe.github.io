---
layout: post
title: python爬虫基础
tags:
- 爬虫
categories: python
description: 爬虫基本类库requests、beautifulsoup、正则表达式re
---

# python爬虫问题汇总
[知乎高野良](https://www.zhihu.com/question/20899988/answer/58388759)

* 基本库：request、beautifulsoup、re
* 现成框架：scrapy
* 动态页面：selenium、phantomJS
* 反爬虫：PIL、opencv、pybrain、打码平台
* 进阶技术：多线程、分布式



# requests库
HTTP请求方法

|方法|含义|
|-|-|
|GET|获取资源|
|POST|传输实体文本|
|HEAD|获取报文首部|
|PUT|传输文件|
|DELETE|删除文件|
|OPTIONS|询问支持的方法|
|TRACE|追踪路径|
|CONNECTION||
|PATCH|部分资源内容更新|


主要方法`requests.request(method, url, **kwargs)`

|-|-|
|method|请求方式，对应get/put/post等7种|
|url|链接|
|**kwargs|控制访问的参数，共13个|

比如：

```python
kv = {'key1': 'value1', 'key2': 'value2'}
r = requests.request('GET', 'http://python123.io/ws', params=kv)
print(r.url)
#http://python123.io/ws?key1=value1&key2=value2
```

最主要的方法requests.get(url)返回一个response对象。

response对象的主要属性

|属性|说明|
|-|-|
|r.status_code|HTTP请求的返回状态，200表示连接成功，404表示失败|
|r.text|HTTP响应内容的字符串形式，即，url对应的页面内容|
|r.encoding|从HTTP header中猜测的响应内容编码方式|
|r.apparent_encoding|从内容中分析出的响应内容编码方式（备选编码方式）|
|r.content|HTTP响应内容的二进制形式|
|r.request.headers|查看发给服务器的头部信息，可以看到python-requests字段|


```python
#最小框架
import requests
def getHTMLText(url):
    try:
        r=requests.get(url)
        r.raise_for_status()#如果状态不是200，引发HTTPError异常
        r.encoding=r.apparent_encoding
        return r.text
    except:
        return '产生异常'

if __name__ == '__main__':
    url='http://www.baidu.com'
    print(getHTMLText(url))
```

小结：
requests库最常用的是get方法来获取页面，对于较大的网页，可以只用head方法获得其头部信息。

# 网络爬虫建议遵守Robots协议
即哪些网站可爬，哪些网站不可爬。Robots协议是建议但非约束性，网络爬虫可以不遵守，但存在法律风险。

所以，爬虫时数据量、商业利益都会增加风险，建议遵守。

# requests库举例

## 解决部分网站阻止爬虫问题

当访问有些网站时，网站可能会组织爬虫，查看`r.request.headers`得到`{'User-Agent': 'python-requests/2.21.0', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive'}`，有些网站会根据这个头部信息阻止本次访问，解决办法是修改头部信息

```python
kv={'user-agent':'Mozilla/5.0'}
url='http://www.niyunsheng.top'
r=requests.get(url,headers=kv)
print(r.status_code)
```

## 从百度搜索获取信息
注意，从360搜索和百度搜索传入参数的关键字不同，应修改。
```python
keywords='python'
kv={'wd':keywords}
url='http://www.baidu.com/s'
r=requests.get(url,params=kv)
print(r)
```
## 获取网络图片

```python
import requests
import os
root="d://pic//"
url='http://img0.dili360.com/ga/M02/49/B7/wKgBzFqo8ySAT4nUAAry7yQ0MW4188.tub.jpg'
path=root+url.split('/')[-1]
try:
    if not os.path.exists(root):#这一句如果注释掉，那，当文件夹不存在时会爬取失败
        os.mkdir(root)
    if not os.path.exists(path):
        r=requests.get(url)
        f=open(path,'wb')
        f.write(r.content)
        f.close()
        print('文件保存成功')
    else:
        print('文件已存在')
except:
    print('爬取失败')
```

## IP地址归属地查询
```python
keywords='58.200.129.130'
kv={'ip':keywords}
url='http://www.ip138.com/ips138.asp'
r=requests.get(url,params=kv)
r.encoding=r.apparent_encoding
print(r.text[-500:])
```

# BeautifulSoup库的安装

BeautifulSoup库对应了html文档的全部内容，是解析、遍历、维护“标签树”的功能库

`<p class="title">hello</p>`
对于每一个标签tag，都有一个名称name和若干个属性attributes（属性以键值对的形式存在，比如，`class="title"`即是一个键值对）

安装`pip install beautifulsoup4`

使用

```python
from bs4 import BeautifulSoup
soup=BeautifulSoup(html_text,'html.parser')
```

第一个参数是html内容，第二个参数是解释器，常用的解释器如下：

|解析器|使用方法|条件|
|-|-|-|
|bs4的HTML解析器|BeautifulSoup(mk,'html.parser')|安装bs4库|
|lxml的HTML解析器|BeautifulSoup(mk,'lxml')|pip install lxml|
|lxml的XML解析器|BeautifulSoup(mk,'xml')|pip install lxml|
|html5lib的解析器|BeautifulSoup(mk,'html5lib')|pip install html5lib|

# Beautiful Soup类的基本元素

|元素|说明|
|-|-|
|Tag|标签，信息组织单元|
|Name|标签的名字，<p></p>的名字是'p'，格式<tag>.name|
|Attributes|标签的属性，字典形式组织，格式<tag>.attrs|
|NavigableString|标签内非属性字符串，`<p>……</p>`中的字符串，格式<tag>.string|
|Comment|标签内字符串的注释部分，一种特殊的Comment类型|

```python
url='https://python123.io/ws/demo.html'
soup.title #可获取页面的title标签中的信息
soup.a #获取页面所有的a标签中的第一个
soup.a.name #获取a标签的名字，当然是'a'了
soup.a.parent.name #获取a标签的父标签
soup.a.attrs #a标签中所有的属性键值对，以字典形式存储
soup.a.attrs['class'] #获取key=class时的值
type(soup.a.attrs) #返回结果是<class 'dict'>，表示返回值是字典类型
soup.a.string
```

# 基于bs4库的HTML内容遍历方法

html是树形结构，有三种遍历方式：上行遍历、下行便利、平行遍历。

需要注意的是，字符串是和标签节点并列的类型。BeautifulSoup类型是标签树的根节点。

|遍历|属性|说明|
|-|-|-|
|下行|.contents|子节点的列表，将<tag>所有儿子节点存入列表|
|下行|.children|子节点的迭代类型，与.contents类似，用于循环遍历儿子节点|
|下行|.descendants|子孙节点的迭代类型，包含所有子孙节点，用于循环遍历|
|上行|.parent|节点的父亲标签|
|上行|.parents|节点先辈标签的迭代类型，用于循环遍历先辈节点|
|平行|.next_sibling|返回按照HTML文本顺序的下一个平行节点标签|
|平行|.previous_sibling|返回按照HTML文本顺序的上一个平行节点标签|
|平行|.next_siblings|迭代类型，返回按照HTML文本顺序的后续所有平行节点标签|
|平行|.previous_siblings|迭代类型，返回按照HTML文本顺序的前续所有平行节点标签|

```python
#b遍历儿子节点
for child in soup.body.children:
	print(child)
#遍历子孙节点
for child in soup.body.descendants:
	print(child)
#遍历父节点
for parent in soup.a.parents:
	if parent is None:
		print(parent)
	else
		print(parent.name)
#遍历平行节点
for sibling in soup.a.next_siblings:
	print(sibling)
for sibling in soup.a.previous_siblings:
	print(sibling)
```

# 基于bs4库的HTML格式化和编码

*更加友好的显示*：`prettify()`方法，为HTML文本<>及其内容增加更加`'\n'`。

例如：`soup.prettify()`或`soup.a.prettify()`

*bs4库的编码*：bs4库将任何HTML输入都变成utf‐8编码。
Python 3.x默认支持编码是utf‐8,解析无障碍。

# 信息标记的三种形式

## XML
eXtensible Markup Language

非空元素`<img src=“china.jpg” size=“10”> … </img>`

空元素`<img src=“china.jpg” size=“10” />`

注释`<!‐‐ This is a comment, very useful ‐‐>`

## JSON
JavsScript Object Notation，有类型的键值对key:value

```json
"name" : {
	"newName" : "北京理工大学",
	"oldName" : "延安自然科学院"
	}
```

这里，name是类型，{}里面的是键值对，多个键值对之间用逗号隔开

## YAML
YAML Ain’t Markup Language，无类型键值对key:value
* 缩进表达所属关系
* `-`表示并列关系
* `|`表达整块数据
* `#`表示注释

```YAML
name:
	newName : 北京理工大学
	oldName : 延安自然科学院
name:
-北京理工大学
-延安自然科学院
text: | #学校介绍
北京理工大学创立于1940年，前身是延安自然科学院，是中国共产党创办的第一所理工科大学。

key : value
key : #Comment
‐value1
‐value2
key :
	subkey : subvalue
```

三种标记形式对比：

|标记形式|比较|
|-|-|
|XML|最早的通用信息标记语言，可扩展性好，但繁琐|
|JSON|信息有类型，适合程序处理(js)，较XML简洁|
|YAML|信息无类型，文本信息比例最高，可读性好|

# 信息提取的一般方法

方法一：完整的解析信息的标记形式，再提取关键信息。例如bs4库的标签树遍历。

优点在于信息解析准确，缺点是过程繁琐，速度慢。

方法二：无视标签形式，直接搜索关键信息

例：提取HTML中所有的URL链接

```python
for link in soup.find_all('a'):
	print(link.get('href'))#这一句可以换做link['href'],因为link是字典类型
```

## BeautifulSoup的find_all方法

`soup.find_all(name, attrs, recursive, string, **kwargs)`

返回一个列表类型，存储查找的结果
* name:对标签名称的检索字符串
* attrs:对标签属性值的检索字符串，可标注属性检索
* recursive: 是否对子孙全部检索，默认True
* string: <>…</>中字符串区域的检索字符串

`<tag>(..)` 等价于`<tag>.find_all(..)`

`soup(..)` 等价于`soup.find_all(..)`

扩展方法


|方法|说明|
|-|-|
|<>.find()| 搜索且只返回一个结果，同.find_all()参数|
|<>.find_parents()| 在先辈节点中搜索，返回列表类型，同.find_all()参数|
|<>.find_parent()| 在先辈节点中返回一个结果，同.find()参数|
|<>.find_next_siblings()| 在后续平行节点中搜索，返回列表类型，同.find_all()参数|
|<>.find_next_sibling()| 在后续平行节点中返回一个结果，同.find()参数|
|<>.find_previous_siblings()| 在前序平行节点中搜索，返回列表类型，同.find_all()参数|
|<>.find_previous_sibling()| 在前序平行节点中返回一个结果，同.find()参数|


# 实例1：中国大学排名定向爬虫

* 爬取页面`http://www.zuihaodaxue.cn/zuihaodaxuepaiming2016.html`
* 检查爬虫可行性之robots协议`http://www.zuihaodaxue.cn/robots.txt`，此网站不存在此文件，所以爬虫可行
* 通过观察html代码，可以发现，所有大学的信息在一个tbody标签中，每个大学的信息在一个tr标签中，之下有若干个td标签，表示标签内的内容
* 代码中体现了用`chr(12288)`来解决中文字符对齐的问题

```python
# 实例1：中国大学排名定向爬虫
import requests
import bs4
from bs4 import BeautifulSoup
import re

def getHTMLText(url):
    try:
        r = requests.get(url)
        r.raise_for_status()  # 如果状态不是200，引发HTTPError异常
        r.encoding = r.apparent_encoding
        return r.text
    except:
        print('产生异常')
        return ''

def fillUnivList(html,list):
    soup=BeautifulSoup(html,'html.parser')
    for tr in soup.find('tbody').children:
        if isinstance(tr,bs4.element.Tag):
            tds=tr('td')
            list.append([tds[0].string,tds[1].string,tds[2].string])

def printUnivList(list,num):
    for rank in range(num):
        print("{0:5}{1:{3}<10}{2}".format(list[rank][0],list[rank][1],list[rank][2],chr(12288)))
    #这里chr(12288)是中文的空格

def main():
    url='http://www.zuihaodaxue.cn/zuihaodaxuepaiming2016.html'
    html=getHTMLText(url)
    list=[]
    fillUnivList(html,list)
    printUnivList(list,20)

main()
```

# 正则表达式(`regular expression, regex, RE`)基本概念

正则表达式是用来简洁表达一组字符串的表达式,用一行来表示表达式的特征或模式。

正则表达式是一种针对字符串表达“简洁”和“特征”思想的工具。
正则表达式可以用来判断某字符串的特征归属。

正则表达式常用操作符

|操作符|说明|实例|
|-|-|-|
|`.`   |表示任何单个字符                  | |
|`[ ]` | 字符集，对单个字符给出取值范围    |[abc]表示a、b、c，[a‐z]表示a到z单个字符|
|`[^ ]`| 非字符集，对单个字符给出排除范围  |[^abc]表示非a或b或c的单个字符|
|`*`   | 前一个字符0次或无限次扩展        |abc* 表示ab、abc、abcc、abccc等|
|`+`   | 前一个字符1次或无限次扩展        |abc+ 表示abc、abcc、abccc等|
|`?`   | 前一个字符0次或1次扩展           |abc? 表示ab、abc|
|`|`   | 左右表达式任意一个               |`abc|def` 表示abc、def|
|`{m}` | 扩展前一个字符m次                |ab{2}c表示abbc,ab{:2}c表示b扩展0次到2次|
|`{m,n}`| 扩展前一个字符m至n次（含n）     | ab{1,2}c表示abc、abbc|
|`^`   | 匹配字符串开头                  |^abc表示abc且在一个字符串的开头|
|`$`   | 匹配字符串结尾                  |abc$表示abc且在一个字符串的结尾|
|`( )` | 分组标记，内部只能使用`|`        | 操作符(abc)表示abc，`(abc|def)`表示abc、def|
|`\d`  | 数字，等价于[0‐9]               | |
|`\w`  | 单词字符，等价于[A‐Za‐z0‐9_]     | |

如何匹配IP地址？分为四段：
* 0-99:`[1-9]?\d`
* 100-199:`1\d{2}`
* 200-249:`2[0-4]\d`
* 250-255:`25[0-5]`
* 0-255:`([1‐9]?\d|1\d{2}|2[0‐4]\d|25[0‐5])`
* `(([1‐9]?\d|1\d{2}|2[0‐4]\d|25[0‐5]).){3}([1‐9]?\d|1\d{2}|2[0‐4]\d|25[0‐5])`


# Re库基本使用
是python的标准库，主要用于字符串匹配。`impoert re`

## 正则表达式的表示类型：raw string（原生字符串）`r'text'`
指的是不包含对转义字符再次进行转义的字符串

re库也可以使用string类型，但更繁琐，要对其中的转义字符加转移符号，如`'[1‐9]\\d{5}'`

## Re库的主要功能函数

|函数       |说明|
|-          |-   |
|re.search()| 在一个字符串中搜索匹配正则表达式的第一个位置，返回match对象|
|re.match()  |从一个字符串的开始位置起匹配正则表达式，返回match对象|
|re.findall()| 搜索字符串，以列表类型返回全部能匹配的子串|
|re.split()  |将一个字符串按照正则表达式匹配结果进行分割，返回列表类型|
|re.finditer()| 搜索字符串，返回一个匹配结果的迭代类型，每个迭代元素是match对象|
|re.sub()     |在一个字符串中替换所有匹配正则表达式的子串，返回替换后的字符串|

`re.search(pattern, string, flags=0)`
* pattern : 正则表达式的字符串或原生字符串表示
* string : 待匹配字符串
* flags : 正则表达式使用时的控制标记

|常用标记     |说明|
|-            |-|
|re.I re.IGNORECASE |忽略正则表达式的大小写，[A‐Z]能够匹配小写字符|
|re.M re.MULTILINE |正则表达式中的^操作符能够将给定字符串的每行当作匹配开始|
|re.S re.DOTALL |正则表达式中的.操作符能够匹配所有字符，默认匹配除换行外的所有字符|


`re.split(pattern, string, maxsplit=0, flags=0)`
* maxsplit: 最大分割数，剩余部分作为最后一个元素输出

`re.sub(pattern, repl, string, count=0, flags=0)`
* repl : 替换匹配字符串的字符串
* count : 匹配的最大替换次数

Re库的两种等价用法
* 函数式用法：一次性操作。`rst = re.search(r'[1‐9]\d{5}', 'BIT 100081')`
* 面向对象用法：编译后的多次操作。`pat = re.compile(r'[1‐9]\d{5}')`,`rst = pat.search('BIT 100081')`

`regex = re.compile(pattern, flags=0)`
* pattern : 正则表达式的字符串或原生字符串表示

## Re库的match对象
Match对象是一次匹配的结果，包含匹配的很多信息。

|属性    |说明|
|-       |-   |
|.string |待匹配的文本|
|.re     |匹配时使用的patter对象（正则表达式）|
|.pos    |正则表达式搜索文本的开始位置|
|.endpos |正则表达式搜索文本的结束位置|

|方法    |说明|
|-       |-  |
|.group(0)| 获得匹配后的字符串|
|.start()  | 匹配字符串在原始字符串的开始位置|
|.end()    |匹配字符串在原始字符串的结束位置|
|.span()   |返回(.start(), .end())|

## Re库的贪婪匹配和最小匹配
默认是贪婪匹配，即同时匹配长短不同的选项时，输出匹配最长的子串。

只要长度输出可能不同的，都可以通过在操作符后增加?变成最小匹配，例如：

|操作符|说明|
|-     |-|
|*?    |前一个字符0次或无限次扩展，最小匹配|
|+?    |前一个字符1次或无限次扩展，最小匹配|
|??    |前一个字符0次或1次扩展，最小匹配|
|{m,n}?| 扩展前一个字符m至n次（含n），最小匹配|




# 实例2：淘宝商品信息定向爬虫
* 目标：获取淘宝搜索页面的信息，提取其中的商品名称和价格
* 理解：
	- 淘宝的搜索接口；
	- 翻页的处理

遗留问题：只能进入淘宝登录页面，反爬虫机制所致，加入了user-agent和cookie才可以运行，但是cookie要加入哪些内容呢？我是把所有cookie的内容全部加入才可以，了解了cookie和淘宝登录之间的机制后，这个问题估计可以迎刃而解。


```python
# 实例2：淘宝商品价格定向查询

import requests
import bs4
from bs4 import BeautifulSoup
import re

def getHTMLText(url):
    try:
        kv={'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:64.0) Gecko/20100101 Firefox/64.0',\
            'Cookie':''}#这里要填入所有字段才可以
        r = requests.get(url,headers=kv)
        r.raise_for_status()  # 如果状态不是200，引发HTTPError异常
        r.encoding = r.apparent_encoding
        return r.text
    except:
        print('产生异常')
        return ''

def parsePage(infoList,html):
    try:
        plt=re.findall(r'\"view_price\"\:\"[\d\.]*\"',html)
        tlt=re.findall(r'\"raw_title\"\:\".*?\"',html)
        for i in range(len(plt)):
            price=eval(plt[i].split(':')[1])
            title=eval(tlt[i].split(':')[1])
            infoList.append([price,title])
    except:
        print("error")

def printGoodsList(infoList):
    count=0
    for info in infoList:
        count=count+1
        print("{}\t{}\t{}".format(count,info[0],info[1]))


    #这里chr(12288)是中文的空格

def main():
    keywords='python'
    start_url='https://s.taobao.com/search?q='+keywords
    infoList=[]
    depth=2
    for i in range(depth):
        try:
            url=start_url+'&s='+str(44*i)
            # print(url)
            html=getHTMLText(url)
            # print(html)
            parsePage(infoList,html)
        except:
            print('error')
            continue
    printGoodsList(infoList)

main()

```


# 实例3：股票数据定向爬虫
* 目的：获取上交所和深交所所有股票的名称和交易信息
* 输出：保存到文件中

```python
# 实例3：股票价格定向查询
import requests
import bs4
from bs4 import BeautifulSoup
import re

def getHTMLText(url,code='utf-8'):
    try:
        kv={'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:64.0) Gecko/20100101 Firefox/64.0',\
            'Cookie':''}
        r = requests.get(url,headers=kv)
        r.raise_for_status()  # 如果状态不是200，引发HTTPError异常
        r.encoding = code
        return r.text
    except:
        # print('getHTMLText Error')
        return ''

def parsePage(stackList,html):
    try:
        soup=BeautifulSoup(html,'html.parser')
        # print(len(soup.find_all(string=re.compile(r'.*\(\d{6}\)'))))
        for str in soup.find_all(string=re.compile(r'.*\(\d{6}\)')):
            tag=str.parent
            # print(type(tag))
            stackList.append([tag.string[:-8],tag.string[-7:-1]])
            # print(tag.string[:-8])
    except:
        print("parsePage Error")

def main():
    listurl='http://quote.eastmoney.com/stocklist.html'
    searchurl='https://gupiao.baidu.com/stock/sh'#+600172.html'
    html=getHTMLText(listurl,'GB2312')

    #获取股票列表
    stackList = []
    parsePage(stackList, html)

    #获取价格信息
    for stack in stackList:
        num=stack[1]
        stackurl=searchurl+num+'.html'
        # print(stackurl)
        html=getHTMLText(stackurl)
        if html=='':
            continue
        soup=BeautifulSoup(html,'html.parser')
        tag=soup.find('strong')
        if(isinstance(tag,bs4.element.Tag)):
            stack.append(tag.string)
            # print("{}:{}:{}".format(stack[0], stack[1], stack[2]))

    #printStackList
    for stack in stackList:
        if len(stack)==3:
            print("{}:{}:{}".format(stack[0],stack[1],stack[2]))
main()


```


优化：提高用户体验
* 添加进度条，使用`\r`将当前光标移动到行首
* 提前计算网页编码，不用apparent_coding，以提高速度。

老师答案：
* 用`tr`和`td`标签获取全部的股票信息，而在自己的代码中仅根据`strong`标签获取了价格信息
* 写入到文件，并增加进度条显示


```python
import requests
from bs4 import BeautifulSoup
import traceback
import re

def getHTMLText(url,coding='utf-8'):
	try:
		r=requests.get(url)
		r.raise_for_status()
		r.encoding=coding
		return r.text
	except:
		return ''

def getStockList(lst,stockURL):
	html=getHTMLText(stockURL,'GB2312')
	soup=BeautifulSoup(html,'html.parser')
	a=soup.find_all('a')
	for i in a:
		try:
			href=i.attrs['href']
			lst.append(re.findall(r'[s][hz]\d{6}',href)[0])
		except:
			continue
	return ''

def getStockInfo(lst,stockURL,fpath):
	count=0
	for stock in lst:
		url=stockURL+stock+'.html'
		html=getHTMLText(url)
		try:
			if html='':
				continue
			infoDict={}
			soup=BeautifulSoup(html,'html.parser')
			stockInfo=soup.find('div',attrs={'class':'stock-bets'})
			name=stockInfo.find_all(attrs={'class':'bets-name'})[0]
			infoDict.update({'股票名称':name.text.split()[0]})

			keyList=stockInfo.find_all('dt')
			valueList=stockInfo.find_all('dd')
			for i in range(len(keyList)):
				key=keyList[i].text
				val=valueList[i].text
				infoDict[key]=value
			with open(fpath,'a',encoding='utf-8') as f:
				f.write(str(infoDict)+'\n')
				count=count+1
				print('\r当前进度:{:.2f}%'.foramt(count*100/len(lst)),end='')
		except：
			count=count+1
			print('\r当前进度:{:.2f}%'.foramt(count*100/len(lst)),end='')
			traceback(str(infoDict)+'\n')
			continue

def main():
	stock_list_url='http://quote.eastmoney.com/stocklist.html'
	stock_info_url='https://gupiao.baidu.com/stock/'
	output_file='d://BaiduStockInof.txt'
	slist=[]
	getStockList(slist,stock_list_url)
	getStockInfo(slist,stock_info_url,output_file)

main()
```
