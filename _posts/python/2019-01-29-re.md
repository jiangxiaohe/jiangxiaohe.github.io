---
layout: post
title: python爬虫入门3-re
tags:
- 爬虫
categories: python
description:
---

# 正则表达式(`regular expression, regex, RE`)基本概念

正则表达式是用来简洁表达一组字符串的表达式,用一行来表示表达式的特征或模式。

正则表达式是一种针对字符串表达“简洁”和“特征”思想的工具。
正则表达式可以用来判断某字符串的特征归属。

正则表达式常用操作符

|操作符|说明|实例|
|-|-|-|
|`.`   |表示任何单个字符                  | |
|`[ ]` | 字符集，对单个字符给出取值范围    |[abc]表示a、b、c，[a‐z]表示a到z单个字符|
|`[^ ]`| 非字符集，对单个字符给出排除范围  |[^abc]表示非a或b或c的单个字符|
|`*`   | 前一个字符0次或无限次扩展        |abc* 表示ab、abc、abcc、abccc等|
|`+`   | 前一个字符1次或无限次扩展        |abc+ 表示abc、abcc、abccc等|
|`?`   | 前一个字符0次或1次扩展           |abc? 表示ab、abc|
|`|`   | 左右表达式任意一个               |`abc|def` 表示abc、def|
|`{m}` | 扩展前一个字符m次                |ab{2}c表示abbc,ab{:2}c表示b扩展0次到2次|
|`{m,n}`| 扩展前一个字符m至n次（含n）     | ab{1,2}c表示abc、abbc|
|`^`   | 匹配字符串开头                  |^abc表示abc且在一个字符串的开头|
|`$`   | 匹配字符串结尾                  |abc$表示abc且在一个字符串的结尾|
|`( )` | 分组标记，内部只能使用`|`        | 操作符(abc)表示abc，`(abc|def)`表示abc、def|
|`\d`  | 数字，等价于[0‐9]               | |
|`\w`  | 单词字符，等价于[A‐Za‐z0‐9_]     | |

如何匹配IP地址？分为四段：
* 0-99:`[1-9]?\d`
* 100-199:`1\d{2}`
* 200-249:`2[0-4]\d`
* 250-255:`25[0-5]`
* 0-255:`([1‐9]?\d|1\d{2}|2[0‐4]\d|25[0‐5])`
* `(([1‐9]?\d|1\d{2}|2[0‐4]\d|25[0‐5]).){3}([1‐9]?\d|1\d{2}|2[0‐4]\d|25[0‐5])`


# Re库基本使用
是python的标准库，主要用于字符串匹配。`impoert re`

## 正则表达式的表示类型：raw string（原生字符串）`r'text'`
指的是不包含对转义字符再次进行转义的字符串

re库也可以使用string类型，但更繁琐，要对其中的转义字符加转移符号，如`'[1‐9]\\d{5}'`

## Re库的主要功能函数

|函数       |说明|
|-          |-   |
|re.search()| 在一个字符串中搜索匹配正则表达式的第一个位置，返回match对象|
|re.match()  |从一个字符串的开始位置起匹配正则表达式，返回match对象|
|re.findall()| 搜索字符串，以列表类型返回全部能匹配的子串|
|re.split()  |将一个字符串按照正则表达式匹配结果进行分割，返回列表类型|
|re.finditer()| 搜索字符串，返回一个匹配结果的迭代类型，每个迭代元素是match对象|
|re.sub()     |在一个字符串中替换所有匹配正则表达式的子串，返回替换后的字符串|

`re.search(pattern, string, flags=0)`
* pattern : 正则表达式的字符串或原生字符串表示
* string : 待匹配字符串
* flags : 正则表达式使用时的控制标记

|常用标记     |说明|
|-            |-|
|re.I re.IGNORECASE |忽略正则表达式的大小写，[A‐Z]能够匹配小写字符|
|re.M re.MULTILINE |正则表达式中的^操作符能够将给定字符串的每行当作匹配开始|
|re.S re.DOTALL |正则表达式中的.操作符能够匹配所有字符，默认匹配除换行外的所有字符|


`re.split(pattern, string, maxsplit=0, flags=0)`
* maxsplit: 最大分割数，剩余部分作为最后一个元素输出

`re.sub(pattern, repl, string, count=0, flags=0)`
* repl : 替换匹配字符串的字符串
* count : 匹配的最大替换次数

Re库的两种等价用法
* 函数式用法：一次性操作。`rst = re.search(r'[1‐9]\d{5}', 'BIT 100081')`
* 面向对象用法：编译后的多次操作。`pat = re.compile(r'[1‐9]\d{5}')`,`rst = pat.search('BIT 100081')`

`regex = re.compile(pattern, flags=0)`
* pattern : 正则表达式的字符串或原生字符串表示

## Re库的match对象
Match对象是一次匹配的结果，包含匹配的很多信息。

|属性    |说明|
|-       |-   |
|.string |待匹配的文本|
|.re     |匹配时使用的patter对象（正则表达式）|
|.pos    |正则表达式搜索文本的开始位置|
|.endpos |正则表达式搜索文本的结束位置|

|方法    |说明|
|-       |-  |
|.group(0)| 获得匹配后的字符串|
|.start()  | 匹配字符串在原始字符串的开始位置|
|.end()    |匹配字符串在原始字符串的结束位置|
|.span()   |返回(.start(), .end())|

## Re库的贪婪匹配和最小匹配
默认是贪婪匹配，即同时匹配长短不同的选项时，输出匹配最长的子串。

只要长度输出可能不同的，都可以通过在操作符后增加?变成最小匹配，例如：

|操作符|说明|
|-     |-|
|*?    |前一个字符0次或无限次扩展，最小匹配|
|+?    |前一个字符1次或无限次扩展，最小匹配|
|??    |前一个字符0次或1次扩展，最小匹配|
|{m,n}?| 扩展前一个字符m至n次（含n），最小匹配|




# 实例2：淘宝商品信息定向爬虫
* 目标：获取淘宝搜索页面的信息，提取其中的商品名称和价格
* 理解：
	- 淘宝的搜索接口；
	- 翻页的处理

遗留问题：只能进入淘宝登录页面，反爬虫机制所致，加入了user-agent和cookie才可以运行，但是cookie要加入哪些内容呢？我是把所有cookie的内容全部加入才可以，了解了cookie和淘宝登录之间的机制后，这个问题估计可以迎刃而解。


```python
# 实例2：淘宝商品价格定向查询

# 实例2：淘宝商品价格定向查询
import requests
import bs4
from bs4 import BeautifulSoup
import re

def getHTMLText(url):
    try:
        kv={'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:64.0) Gecko/20100101 Firefox/64.0',\
            'Cookie':''}#这里要填入所有字段才可以
        r = requests.get(url,headers=kv)
        r.raise_for_status()  # 如果状态不是200，引发HTTPError异常
        r.encoding = r.apparent_encoding
        return r.text
    except:
        print('产生异常')
        return ''

def parsePage(infoList,html):
    try:
        plt=re.findall(r'\"view_price\"\:\"[\d\.]*\"',html)
        tlt=re.findall(r'\"raw_title\"\:\".*?\"',html)
        for i in range(len(plt)):
            price=eval(plt[i].split(':')[1])
            title=eval(tlt[i].split(':')[1])
            infoList.append([price,title])
    except:
        print("error")

def printGoodsList(infoList):
    count=0
    for info in infoList:
        count=count+1
        print("{}\t{}\t{}".format(count,info[0],info[1]))


    #这里chr(12288)是中文的空格

def main():
    keywords='python'
    start_url='https://s.taobao.com/search?q='+keywords
    infoList=[]
    depth=2
    for i in range(depth):
        try:
            url=start_url+'&s='+str(44*i)
            # print(url)
            html=getHTMLText(url)
            # print(html)
            parsePage(infoList,html)
        except:
            print('error')
            continue
    printGoodsList(infoList)

main()

```


# 实例3：股票数据定向爬虫
* 目的：获取上交所和深交所所有股票的名称和交易信息
* 输出：保存到文件中

```python
# 实例3：股票价格定向查询
import requests
import bs4
from bs4 import BeautifulSoup
import re

def getHTMLText(url,code='utf-8'):
    try:
        kv={'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:64.0) Gecko/20100101 Firefox/64.0',\
            'Cookie':''}
        r = requests.get(url,headers=kv)
        r.raise_for_status()  # 如果状态不是200，引发HTTPError异常
        r.encoding = code
        return r.text
    except:
        # print('getHTMLText Error')
        return ''

def parsePage(stackList,html):
    try:
        soup=BeautifulSoup(html,'html.parser')
        # print(len(soup.find_all(string=re.compile(r'.*\(\d{6}\)'))))
        for str in soup.find_all(string=re.compile(r'.*\(\d{6}\)')):
            tag=str.parent
            # print(type(tag))
            stackList.append([tag.string[:-8],tag.string[-7:-1]])
            # print(tag.string[:-8])
    except:
        print("parsePage Error")

def main():
    listurl='http://quote.eastmoney.com/stocklist.html'
    searchurl='https://gupiao.baidu.com/stock/sh'#+600172.html'
    html=getHTMLText(listurl,'GB2312')

    #获取股票列表
    stackList = []
    parsePage(stackList, html)

    #获取价格信息
    for stack in stackList:
        num=stack[1]
        stackurl=searchurl+num+'.html'
        # print(stackurl)
        html=getHTMLText(stackurl)
        if html=='':
            continue
        soup=BeautifulSoup(html,'html.parser')
        tag=soup.find('strong')
        if(isinstance(tag,bs4.element.Tag)):
            stack.append(tag.string)
            # print("{}:{}:{}".format(stack[0], stack[1], stack[2]))

    #printStackList
    for stack in stackList:
        if len(stack)==3:
            print("{}:{}:{}".format(stack[0],stack[1],stack[2]))
main()


```


优化：提高用户体验
* 添加进度条，使用`\r`将当前光标移动到行首
* 提前计算网页编码，不用apparent_coding，以提高速度。
