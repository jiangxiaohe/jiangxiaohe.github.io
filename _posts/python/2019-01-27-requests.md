---
layout: post
title: python爬虫入门1-requests
tags:
- 爬虫
categories: python
description: 爬虫基本类库requests
---
# requests库
HTTP请求方法

|方法|含义|
|-|-|
|GET|获取资源|
|POST|传输实体文本|
|HEAD|获取报文首部|
|PUT|传输文件|
|DELETE|删除文件|
|OPTIONS|询问支持的方法|
|TRACE|追踪路径|
|CONNECTION||
|PATCH|部分资源内容更新|


主要方法`requests.request(method, url, **kwargs)`

|-|-|
|method|请求方式，对应get/put/post等7种|
|url|链接|
|**kwargs|控制访问的参数，共13个|

比如：
```python
kv = {'key1': 'value1', 'key2': 'value2'}
r = requests.request('GET', 'http://python123.io/ws', params=kv)
print(r.url)
#http://python123.io/ws?key1=value1&key2=value2
```

最主要的方法requests.get(url)返回一个response对象。

response对象的主要属性

|属性|说明|
|-|-|
|r.status_code|HTTP请求的返回状态，200表示连接成功，404表示失败|
|r.text|HTTP响应内容的字符串形式，即，url对应的页面内容|
|r.encoding|从HTTP header中猜测的响应内容编码方式|
|r.apparent_encoding|从内容中分析出的响应内容编码方式（备选编码方式）|
|r.content|HTTP响应内容的二进制形式|
|r.request.headers|查看发给服务器的头部信息，可以看到python-requests字段|


```python
#最小框架
import requests
def getHTMLText(url):
    try:
        r=requests.get(url)
        r.raise_for_status()#如果状态不是200，引发HTTPError异常
        r.encoding=r.apparent_encoding
        return r.text
    except:
        return '产生异常'

if __name__ == '__main__':
    url='http://www.baidu.com'
    print(getHTMLText(url))
```

小结：
requests库最常用的是get方法来获取页面，对于较大的网页，可以只用head方法获得其头部信息。



# 网络爬虫建议遵守Robots协议
即哪些网站可爬，哪些网站不可爬。Robots协议是建议但非约束性，网络爬虫可以不遵守，但存在法律风险。

所以，爬虫时数据量、商业利益都会增加风险，建议遵守。

# requests库举例

## 解决部分网站阻止爬虫问题

当访问有些网站时，网站可能会组织爬虫，查看`r.request.headers`得到`{'User-Agent': 'python-requests/2.21.0', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive'}`，有些网站会根据这个头部信息阻止本次访问，解决办法是修改头部信息

```python
kv={'user-agent':'Mozilla/5.0'}
url='http://www.niyunsheng.top'
r=requests.get(url,headers=kv)
print(r.status_code)
```

## 从百度搜索获取信息
注意，从360搜索和百度搜索传入参数的关键字不同，应修改。
```python
keywords='python'
kv={'wd':keywords}
url='http://www.baidu.com/s'
r=requests.get(url,params=kv)
print(r)
```
## 获取网络图片

```python
import requests
import os
root="d://pic//"
url='http://img0.dili360.com/ga/M02/49/B7/wKgBzFqo8ySAT4nUAAry7yQ0MW4188.tub.jpg'
path=root+url.split('/')[-1]
try:
    if not os.path.exists(root):#这一句如果注释掉，那，当文件夹不存在时会爬取失败
        os.mkdir(root)
    if not os.path.exists(path):
        r=requests.get(url)
        f=open(path,'wb')
        f.write(r.content)
        f.close()
        print('文件保存成功')
    else:
        print('文件已存在')
except:
    print('爬取失败')
```

## IP地址归属地查询
```python
keywords='58.200.129.130'
kv={'ip':keywords}
url='http://www.ip138.com/ips138.asp'
r=requests.get(url,params=kv)
r.encoding=r.apparent_encoding
print(r.text[-500:])
```
