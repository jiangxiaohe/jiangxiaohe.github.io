---
layout: post
title: Hadoop实验
tags:
categories: tools
description:
---

环境：搭建Docker伪分布式集群环境

# 在ubuntu搭建Docker环境

参考[CSDN教程](https://blog.csdn.net/yx_222/article/details/80936757)

[官方安装教程](https://docs.docker.com/install/linux/docker-ce/ubuntu/#set-up-the-repository)

[docker命令大全](https://www.runoob.com/docker/docker-command-manual.html)

```
//为了使apt可以通过https使用Repository，先安装以下包
sudo apt-get install -y apt-transport-https ca-certificates curl software-properties-common
//添加Docker官方GPG密钥
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
//检查GPG Key信息是否正确
sudo apt-key fingerprint 0EBFCD88
//将源信息直接写入/etc/apt/sources.list
sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"
//再更新下apt包索引
sudo apt-get update
//确认Docker的源信息是否正确, 新的源是否添加成功
sudo apt-cache madison docker-ce
//安装最新版本的Docker CE
sudo apt-get install -y docker-ce
//Docker安装验证
sudo docker -v
sudo docker version

//启动docker
sudo systemctl start docke
//运行Hello World，校验Docker是否安装成功
sudo docker run hello-world
```

docker常用命令
* docker images列出所有镜像
* docker ps列出正在运行的容器，-a :显示所有的容器，包括未运行的
* docker pull docker-spark-hadoop 下载镜像
* docker run -i -t ubuntu /bin/bash
  * -i: 以交互模式运行容器，通常与 -t 同时使用
  * -t: 为容器重新分配一个伪输入终端，通常与 -i 同时使用；
  * -h "mars": 指定容器的hostname
  * --name="nginx-lb": 为容器指定一个名称
* docker commit 在本地仓库提交更改(类似git)
* docker rm contrainer-id 删除容器
* docker attach container 连接正在运行的容器
* docker start container 启动一个或多个已经被停止的容器

## [本地访问Docker端口](https://www.cnblogs.com/kevingrace/p/9453987.html)

默认情况下，Docker守护进程会生成一个socket（/var/run/docker.sock）文件来进行本地进程通信，而不会监听任何端口，因此只能在本地使用docker客户端或者使用Docker API进行操作。

* 最好的方法就是在启动时通过-p命令设置容器和主机的ip映射

但是，如果容器启动时没有指定端口映射，也可以通过宿主机的iptables进行nat转发访问容器内部端口。

但是 `sudo docker ps` 发现容器的80端口并没有打开，所以无法通过telnet访问并且添加端口映射来实现。

* 最后的结论是只有在创建模型时至少打开一个端口(比如80)，然后才能实现互联。

# docker环境下搭建hadoop-spark集群(ubuntu16.04 LTS系统）

## 下载hadoop镜像并且配置ssh互联

[参考CSDN](https://blog.csdn.net/wangzi11111111/article/details/88890988)

* 从阿里云库拉取hadoop镜像，下载镜像registry.cn-beijing.aliyuncs.com/bitnp/docker-spark-hadoop，这个镜像把我们需要的工具基本下好了，比如 jdk、hadoop、spark。
`docker pull registry.cn-beijing.aliyuncs.com/bitnp/docker-spark-hadoop`

注意，可以用`cat /etc/redhat-release`显示该系统为CentOS系统，如果找不到文件则是ubuntu系统。

* 使用指令`docker images`查看是否下载成功

* 在这个docker镜像中创建是三个容器(Master Slave1 Slave2)
* 创建matser容器：`docker run -it --name Master -h Master registry.cn-beijing.aliyuncs.com/bitnp/docker-spark-hadoop /bin/bash`
* 此时，就把Master容器创建出来了，按键`ctrl+P+Q`会返回主机的命令行，但是并不会退出Master容器，如果用`ctrl+c`则会退出Master容器。
* 同样，继续创建slave1和slave2容器。
`docker run -it --name Slave1 -h Slave1 registry.cn-beijing.aliyuncs.com/bitnp/docker-spark-hadoop /bin/bash`
* 至此，三个容器已经创建完毕，接下来需要通过ssh将三个容器连接起来。

* 首先对Master进行配置，进入Master容器`docker attach Master`
* docker里面没有指令apt-get，没有指令gedit，但是里面有yum，可以用yum下载vim来编辑文件，还可以用yum下载openssh-clients，openssh-server.如果你在docker里面连yum都没有，那么你先使用Ctrl+P+Q退出，在初始目录用apt-get下载一个yum(指令是  sudo apt-get  install yum )，然后在docker里面就可以使用了。
* `yum -y install vim` `yum -y install openssh-clients` `yum -y install openssh-server`
* 配置Master容器的ssh秘钥
  * `/usr/sbin/sshd`
  * `/usr/sbin/sshd-keygen -A`
  * `/usr/sbin/sshd`
  * 三步不可少，不然会出问题
* 制作秘钥`ssh-keygen -t rsa`，默认生成`/root/.ssh/id_rsa.pub`
* 我们需要把密钥存储在 /root/.ssh/authorized_keys 文件中，指令是： `cat /root/.ssh/id_rsa.pub >> /root/.ssh/authorized_keys`
* 为了使得ssh连接时更加简洁，需要配置两个文件
```cpp
vim /etc/ssh/sshd_config
'''
Port 22
PermitRootLogin yes
PubkeyAuthentication yes
PasswordAuthentication yes
ChallengeResponseAuthentication no
UsePAM yes
PrintLastLog no
'''
vim /etc/ssh/ssh_config
'''
StrictHostKeyChecking no
'''
```

* 新开一个命令行界面，看下三个容器的ip地址`sudo docker inspect --format='{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' $(sudo docker ps -aq) `可以看到ip地址分别为
`/Master - 172.17.0.2
/Slave1 - 172.17.0.3
/Slave2 - 172.17.0.4`

* 在Master容器中，打开 /etc/hosts 文件，把上述内容填上，目的是给每个节点的 ip 附上名字，ssh连接的时候就可以直接  ssh Slave1，而不是 ssh 172.17.0.3 这么麻烦了。` vim /etc/hosts`
```
172.17.0.2      Master
172.17.0.3      Slave1
172.17.0.4      Slave2
```
* 最后一步，将秘钥文件存入另外两个机器的文件当中，直接复制即可。`vim /root/.ssh/authorized_keys`

## 配置环境变量
* 首先查看JAVE_HOME的地址`echo $JAVA_HOME`
* 接下来一次配置每个容器的 core-site.xml 和 yarn-site.xml 和 mapred-site.xml 及 hdfs-site.xml 文件
* 首先使用 find / -name core-site.xml 的具体路径，然后用指令  vim + 文件路径  进入这个文件.`vim /usr/local/hadoop-2.7.5/etc/hadoop/core-site.xml`

```
<!-- Put site-specific property overrides in this file. -->
<configuration>
      <property>
          <name>fs.defaultFS</name>
          <value>hdfs://Master:9000</value>
      </property>
      <property>
         <name>io.file.buffer.size</name>
         <value>131072</value>
     </property>
     <property>
          <name>hadoop.tmp.dir</name>
          <value>/usr/local/hadoop-2.7.5/tmp</value>
     </property>
</configuration>
```

* 进入yarn-site.xml 进行配置，结束后保存退出。`vim /usr/local/hadoop-2.7.5/etc/hadoop/yarn-site.xml`

```
<configuration>
     <property>
         <name>yarn.nodemanager.aux-services</name>
         <value>mapreduce_shuffle</value>
     </property>
     <property>
         <name>yarn.resourcemanager.address</name>
         <value>Master:8032</value>
     </property>
     <property>
         <name>yarn.resourcemanager.scheduler.address</name>
         <value>Master:8030</value>
     </property>
     <property>
         <name>yarn.resourcemanager.resource-tracker.address</name>
         <value>Master:8031</value>
     </property>
     <property>
         <name>yarn.resourcemanager.admin.address</name>
         <value>Master:8033</value>
     </property>
     <property>
         <name>yarn.resourcemanager.webapp.address</name>
         <value>Master:8088</value>
     </property>
     <property>
         <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
         <value>org.apache.hadoop.mapred.ShuffleHandler</value>
     </property>
</configuration>
```

* 进入mapred-site.xml  进行配置，结束后保存退出。`vim /usr/local/hadoop-2.7.5/etc/hadoop/mapred-site.xml`

```
<!-- Put site-specific property overrides in this file. -->
<configuration>
 <property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
  </property>
</configuration>
```

* 进入hdfs-site.xml  进行配置，结束后保存退出。`vim /usr/local/hadoop-2.7.5/etc/hadoop/hdfs-site.xml`

```
<!-- Put site-specific property overrides in this file. -->
<configuration>
    <property>
      <name>dfs.replication</name>
      <value>2</value>
    </property>
    <property>
      <name>dfs.namenode.name.dir</name>
      <value>file:/usr/local/hadoop-2.7.5/hdfs/name</value>
    </property>
</configuration>
```

* 在步骤I 中是配置 Master 容器的环境变量，我们还需要进入Slave1容器，相同的代码把 Slave1的环境变量也配置完，当然容器slave2也是如此。唯一不同的是在步骤⑤ 的hdfs-site.xml中，Master容器设置的是namenode，而Slave1和Slave2设置的是datanode，如下:

```
<!-- Put site-specific property overrides in this file. -->
<configuration>
    <property>
      <name>dfs.replication</name>
      <value>2</value>
    </property>
    <property>
      <name>dfs.datanode.data.dir</name>
      <value>file:/usr/local/hadoop-2.7.5/hdfs/data</value>
    </property>
</configuration>
```

* 删除三个容器的hdfs目录`rm -rf /usr/local/hadoop-2.7.5/hdfs`
* 在Master下简历name文件夹`mkdir -p /usr/local/hadoop-2.7.5/hdfs/name`
* 在Slave下简历data文件夹`mkdir -p /usr/local/hadoop-2.7.5/hdfs/data`
* 格式化NameNode HDFS目录，在Master容器中，使用指令 `hdfs namenode -format`
* 进入sbin文件，来启动hadoop集群。`cd /usr/local/hadoop-2.7.5/sbin`
* 然后我们使用指令 `./start-all.sh`  来启动集群.

```
这是出现了报错信息：
./start-all.sh
This script is Deprecated. Instead use start-dfs.sh and start-yarn.sh
Starting namenodes on [master]
master: starting namenode, logging to /usr/local/hadoop/logs/hadoop-root-namenode-master.out
spark-slave1: ssh: Could not resolve hostname spark-slave1: Name or service not known
spark-slave2: ssh: Could not resolve hostname spark-slave2: Name or service not known
Starting secondary namenodes [0.0.0.0]
0.0.0.0: starting secondarynamenode, logging to /usr/local/hadoop/logs/hadoop-root-secondarynamenode-master.out
starting yarn daemons
starting resourcemanager, logging to /usr/local/hadoop/logs/yarn--resourcemanager-master.out
spark-slave2: ssh: Could not resolve hostname spark-slave2: Name or service not known
spark-slave1: ssh: Could not resolve hostname spark-slave1: Name or service not known

原因在于无法解析spark-slave1和spark-slave2主机名，原来是还需要配置文件设置子节点
```

* 设置子节点`vim /usr/local/hadoop/etc/hadoop/slaves`,设置为自己的子节点
* 使用 jps 查看 namenode 是否启动，此时看的是Master容器的namenode是否启动。
* 这里我们可以使用  ssh Slave1  (或ssh Slave2)进入Slave1容器，然后使用指令  jps  查看datanode是否启动，此时会出现`-bash: jps: command not found` 错误，需要配置` /etc/profile`文件，三个容器都要配置。在末尾添加，添加后执行`source /etc/profile`

```
export JAVA_HOME=/usr/local/jdk1.8.0_162
export HADOOP_HOME=/usr/local/hadoop-2.7.5
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
```

* 可以使用指令 hadoop dfsadmin -report   查看其他容器有没有启动

可以在
* 查看节点状态：主机ip:50070/dfshealth.html#tab-datanode
* HDFS文件目录： 主机ip:50070/explorer.html#/
* 任务监控： 主机ip:8088/cluster/apps
* 节点运行状态： 主机ip:8088/cluster/nodes

无奈的是本次没有在启动容器时设置端口映射，故无法亲自查看端口

# hadoop实验

## [`hdfs dfs`命令](https://blog.csdn.net/afafawfaf/article/details/80254989)

安装完hadoop之后可以使用hdfs命令，其位置在`/usr/local/hadoop/bin/hdfs`,`hdfs dfs`命令的含义是操作hadoop上的一个分布式文件系统。

* * hdfs dfs命令使用范围更广，可以用于其他文件系统，不止是hdfs文件系统
* hadoop dfs专门针对hdfs分布式文件系统，不推荐使用
* hdfs dfs推荐使用

* hdfs dfs -ls  显示当前目录结构，-ls -R 递归显示目录结构
* hdfs dfs -mkdir  创建目录
* hdfs dfs -rm   删除文件，-rm -R 递归删除目录和文件
* hdfs dfs -put  [localsrc] [dst]  从本地加载文件到HDFS
* hdfs dfs -get  [dst] [localsrc]  从HDFS导出文件到本地
* hdfs dfs - copyFromLocal [localsrc] [dst]  从本地加载文件到HDFS，与put一致
* hdfs dfs -copyToLocal [dst] [localsrc]  从HDFS导出文件到本地，与get一致
* hdfs dfs -test -e  检测目录和文件是否存在，存在返回值$?为0，不存在返回1
* hdfs dfs -text  查看文件内容
* hdfs dfs -du  统计目录下各文件大小，单位字节。-du -s 汇总目录下文件大小，-du -h 显示单位
* hdfs dfs -tail  显示文件末尾
* hdfs dfs -cp [src] [dst] 从源目录复制文件到目标目录
* hdfs dfs -mv [src] [dst] 从源目录移动文件到目标目录

## [运行Hadoop实例程序grep](http://dblab.xmu.edu.cn/blog/1233/)
该程序实现的是对指定文档中指定单词的词频进行计算。

进入hadoop文件夹`/usr/local/hadoop-2.7.5`，注意这里实在分布式环境中，需要用`hdfs dfs`命令，否则会报错
* 创建input目录`hdfs dfs -mkdir -p /user/hadoop/input`
* 将etc目录下所有的xml复制到input目录下`hdfs dfs -put ./etc/hadoop/*.xml /user/hadoop/input`
* 通过ls命令查看下是否正确将文件上传到hdfs下
* 运行hadoop命令。
`./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar grep /user/hadoop/input output 'dfs[a-z.]+'`
  * `./bin/hadoop`启动hadoop命令
  * `jar`以jar包运行方式运行
  *  `share/hadoop/mapreduce-examles-2.7.5.jar`官网提供的案例目录
  * `grep`官方提供的grep案例，其他的如wordcount模型等
  * `input output`输入输出目录（千万不要自己创建output目录、hadoop源码中会判断这个文件是否存在、存在的话会报异常）
  * `dfs[a-z.]+`正则表达式 表示已dfs开头所有匹配项，可无限累加
* 运行完毕，可以在hdfs的output目录下查看运行结果`hdfs dfs -cat output/*`

## [运行Hadoop实例程序wordcount](https://blog.csdn.net/l1394049664/article/details/82562535)

在一堆给定的文本文件中统计输出每一个单词出现的总次数
* 准备数据文件
```
word1.txt
i love the world
word2.txt
i love the world , hello ni hao
```
* 将该文件放入hdfs的input文件夹，首先删除上个例程中的input，`hdfs dfs -rm /user/hadoop/input/*`
* 删除上个程序创建的output文件夹`hdfs dfs -rm -r output`
* 将数据文件移入input文件夹`hdfs dfs -put word*.txt /user/hadoop/input`
* 运行命令
`./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar wordcount /user/hadoop/input output`
* 查看结果`hdfs dfs -ls output`，进而看到结果在`hdfs dfs -cat output/part-r-0000`

## [自己编写wordcount案例](https://www.cnblogs.com/frankdeng/p/9256254.html)

需要自己编写mapper、reduce、driver类。

## 总结

* 每次运行程序之前先要启动hadoop集群(请按照前文配置和启动)
* 执行hadoop程序
* 关闭hadoop集群，在`/usr/local/hadoop-2.7.5/sbin`目录下运行`./stop-all.sh`
* 保存这三个hadoop容器，这里运行完程序后只需要停止容器即可，不用删除容器，以备下次使用。这里配置三个容器这么不容易，可不能轻易删除了。
* 重启后的问题
  * 重启后需要重新配置ip。`vim /etc/hosts`
  * 重启后发现ssh服务没有打开，由于docker里面的是centos，所以service和systemctl都不好用，需要手动运行`/usr/sbin/sshd`
    * 用`ssh localhost`命令来判断ssh是否启动
    * 用`rpm -qa | grep ssh`查看ssh是否安装
  * 删除datanode其hdfs/data下面的文件 `rm -rf /usr/local/hadoop-2.7.5/hdfs/data/*`
  * 删除namenode其hdfs/name下面的文件 `rm -rf /usr/local/hadoop-2.7.5/hdfs/name/*`
  * 重新启动hdfs需要格式化namenode，在master机器上运行命令 `hdfs namenode -format`
  * 用`jps`和`hdfs dfsadmin -report`查看是否启动

# spark

待完成！

http://spark.apache.org/docs/latest/configuration.html#inheriting-hadoop-cluster-configuration

To make these files visible to Spark, set HADOOP_CONF_DIR in $SPARK_HOME/conf/spark-env.sh to a location containing the configuration files.
